{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1e61961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import warnings\n",
    "import gc\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple\n",
    "import pickle\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import hashlib\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "# Core data science libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, classification_report, confusion_matrix,\n",
    "    mean_absolute_percentage_error, precision_recall_curve\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PowerTransformer, QuantileTransformer, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Catboost\n",
    "import catboost as cb\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "\n",
    "# Optuna for hyperparameter optimization\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# Added tensorflow libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b6ae91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'plots' is ready.\n",
      "Directory 'processed_data' is ready.\n"
     ]
    }
   ],
   "source": [
    "# Define the plot output path\n",
    "plot_output_path = Path('plots')\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "plot_output_path.mkdir(exist_ok=True)\n",
    "print(f\"Directory '{plot_output_path}' is ready.\")\n",
    "\n",
    "# Define processed data output path\n",
    "processed_data_output_path = Path('processed_data')\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "processed_data_output_path.mkdir(exist_ok=True)\n",
    "print(f\"Directory '{processed_data_output_path}' is ready.\")\n",
    "\n",
    "model_output_path = Path('best_model')\n",
    "model_output_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d80d6ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized physics calculations using vectorization\n",
    "def remove_gravity_from_acc_vectorized(acc_values, quat_values):\n",
    "    \"\"\"Vectorized gravity removal for better performance\"\"\"\n",
    "    num_samples = acc_values.shape[0]\n",
    "    linear_accel = acc_values.copy()\n",
    "    \n",
    "    # Filter valid quaternions\n",
    "    valid_mask = ~(np.any(np.isnan(quat_values), axis=1) | \n",
    "                   np.all(np.isclose(quat_values, 0), axis=1))\n",
    "    \n",
    "    if np.any(valid_mask):\n",
    "        # Process all valid quaternions at once\n",
    "        valid_quats = quat_values[valid_mask]\n",
    "        \n",
    "        # Batch rotation computation\n",
    "        try:\n",
    "            rotations = R.from_quat(valid_quats)\n",
    "            gravity_world = np.array([0, 0, 9.81])\n",
    "            \n",
    "            # Apply rotations in batch\n",
    "            gravity_sensor_frames = rotations.apply(gravity_world, inverse=True)\n",
    "            linear_accel[valid_mask] = acc_values[valid_mask] - gravity_sensor_frames\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    return linear_accel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "856b6520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angular_velocity_vectorized(quat_values, time_delta=1/200):\n",
    "    \"\"\"Vectorized angular velocity calculation\"\"\"\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_vel = np.zeros((num_samples, 3))\n",
    "    \n",
    "    if num_samples < 2:\n",
    "        return angular_vel\n",
    "        \n",
    "    # Process in chunks for memory efficiency\n",
    "    chunk_size = 1000\n",
    "    for start in range(0, num_samples - 1, chunk_size):\n",
    "        end = min(start + chunk_size, num_samples - 1)\n",
    "        \n",
    "        q_t = quat_values[start:end]\n",
    "        q_t_plus_dt = quat_values[start+1:end+1]\n",
    "        \n",
    "        # Find valid pairs\n",
    "        valid_mask = ~(np.any(np.isnan(q_t), axis=1) | \n",
    "                      np.all(np.isclose(q_t, 0), axis=1) |\n",
    "                      np.any(np.isnan(q_t_plus_dt), axis=1) | \n",
    "                      np.all(np.isclose(q_t_plus_dt, 0), axis=1))\n",
    "        \n",
    "        if np.any(valid_mask):\n",
    "            try:\n",
    "                valid_indices = np.where(valid_mask)[0]\n",
    "                rot_t = R.from_quat(q_t[valid_mask])\n",
    "                rot_t_plus_dt = R.from_quat(q_t_plus_dt[valid_mask])\n",
    "                \n",
    "                # Batch computation\n",
    "                delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "                angular_vel[start + valid_indices] = delta_rot.as_rotvec() / time_delta\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "    return angular_vel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "039c2c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel processing for sequence features\n",
    "def process_sequence(seq_data):\n",
    "    \"\"\"Process a single sequence with all feature engineering\"\"\"\n",
    "    seq_data = seq_data.copy()\n",
    "    \n",
    "    # Get numpy arrays for faster processing\n",
    "    acc_values = seq_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    quat_values = seq_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "\n",
    "    try:\n",
    "        rotations = R.from_quat(quat_values)\n",
    "        euler_angles = rotations.as_euler('xyz', degrees=True)\n",
    "        seq_data['euler_x'] = euler_angles[:, 0]\n",
    "        seq_data['euler_y'] = euler_angles[:, 1]\n",
    "        seq_data['euler_z'] = euler_angles[:, 2]\n",
    "    except:\n",
    "        seq_data['euler_x'] = 0\n",
    "        seq_data['euler_y'] = 0\n",
    "        seq_data['euler_z'] = 0\n",
    "    \n",
    "    # Linear acceleration\n",
    "    linear_accel = remove_gravity_from_acc_vectorized(acc_values, quat_values)\n",
    "    seq_data['linear_acc_x'] = linear_accel[:, 0]\n",
    "    seq_data['linear_acc_y'] = linear_accel[:, 1]\n",
    "    seq_data['linear_acc_z'] = linear_accel[:, 2]\n",
    "\n",
    "    # Acceleration features focusing on z and y axes\n",
    "    seq_data['acc_yz_mag'] = np.sqrt(seq_data['acc_y']**2 + seq_data['acc_z']**2)\n",
    "    seq_data['acc_y_z_ratio'] = seq_data['acc_y'] / (seq_data['acc_z'] + 1e-8)\n",
    "    \n",
    "    # Magnitudes\n",
    "    seq_data['acc_mag'] = np.linalg.norm(acc_values, axis=1)\n",
    "    seq_data['linear_acc_mag'] = np.linalg.norm(linear_accel, axis=1)\n",
    "    \n",
    "    # Jerk (simplified)\n",
    "    seq_data['linear_acc_mag_jerk'] = np.gradient(seq_data['linear_acc_mag']) * 200\n",
    "    \n",
    "    # Angular velocity\n",
    "    angular_vel = calculate_angular_velocity_vectorized(quat_values)\n",
    "    seq_data['angular_vel_x'] = angular_vel[:, 0]\n",
    "    seq_data['angular_vel_y'] = angular_vel[:, 1]\n",
    "    seq_data['angular_vel_z'] = angular_vel[:, 2]\n",
    "    \n",
    "    # ToF aggregations (vectorized)\n",
    "    for i in range(1, 6):\n",
    "        pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "        tof_data = seq_data[pixel_cols].values\n",
    "        tof_data[tof_data == -1] = np.nan\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            seq_data[f'tof_{i}_mean'] = np.nanmean(tof_data, axis=1)\n",
    "            seq_data[f'tof_{i}_std'] = np.nanstd(tof_data, axis=1)\n",
    "            seq_data[f'tof_{i}_min'] = np.nanmin(tof_data, axis=1)\n",
    "            seq_data[f'tof_{i}_max'] = np.nanmax(tof_data, axis=1)\n",
    "            seq_data[f'tof_{i}_var'] = np.nanvar(tof_data, axis=1)\n",
    "\n",
    "    # Add rotation-specific features for rot_z and rot_w\n",
    "    seq_data['rot_z_w_product'] = seq_data['rot_z'] * seq_data['rot_w']\n",
    "    seq_data['rot_z_w_ratio'] = seq_data['rot_z'] / (seq_data['rot_w'] + 1e-8)\n",
    "\n",
    "    # Rolling statistics for important features (window of 10 samples = 50ms)\n",
    "    for col in ['rot_z', 'rot_w', 'acc_z', 'acc_y', 'thm_2']:\n",
    "        seq_data[f'{col}_rolling_mean'] = seq_data[col].rolling(10, center=True).mean()\n",
    "        seq_data[f'{col}_rolling_std'] = seq_data[col].rolling(10, center=True).std()\n",
    "        seq_data[f'{col}_diff'] = seq_data[col].diff()\n",
    "    \n",
    "    # Thermopile 2 specific features\n",
    "    seq_data['thm_2_normalized'] = (seq_data['thm_2'] - seq_data['thm_2'].mean()) / (seq_data['thm_2'].std() + 1e-8)\n",
    "    seq_data['thm_2_delta_from_mean'] = seq_data['thm_2'] - seq_data[['thm_1', 'thm_2', 'thm_3', 'thm_4', 'thm_5']].mean(axis=1)\n",
    "    \n",
    "    # ToF band statistics for tof_1 and tof_2 (statistically significant bands)\n",
    "    bands_tof_1_2 = [\n",
    "        (3, 7),    # Band 1: v[3-7]\n",
    "        (11, 15),  # Band 2: v[11-15]\n",
    "        (19, 23),  # Band 3: v[19-23]\n",
    "        (27, 31),  # Band 4: v[27-31]\n",
    "        (35, 39),  # Band 5: v[35-39]\n",
    "        (43, 47),  # Band 6: v[43-47]\n",
    "        (51, 55),  # Band 7: v[51-55]\n",
    "        (59, 63),  # Band 8: v[59-63]\n",
    "    ]\n",
    "    \n",
    "    for tof_num in [1, 2]:  # Only process tof_1 and tof_2\n",
    "        for band_idx, (start, end) in enumerate(bands_tof_1_2, 1):\n",
    "            # Get columns for this band (inclusive range)\n",
    "            band_cols = [f\"tof_{tof_num}_v{p}\" for p in range(start, end + 1)]\n",
    "            band_data = seq_data[band_cols].values\n",
    "            band_data[band_data == -1] = np.nan\n",
    "            \n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                seq_data[f'tof_{tof_num}_band{band_idx}_mean'] = np.nanmean(band_data, axis=1)\n",
    "                seq_data[f'tof_{tof_num}_band{band_idx}_std'] = np.nanstd(band_data, axis=1)\n",
    "                seq_data[f'tof_{tof_num}_band{band_idx}_var'] = np.nanvar(band_data, axis=1)\n",
    "                seq_data[f'tof_{tof_num}_band{band_idx}_min'] = np.nanmin(band_data, axis=1)\n",
    "                seq_data[f'tof_{tof_num}_band{band_idx}_max'] = np.nanmax(band_data, axis=1)\n",
    "                \n",
    "    # ToF band statistics for tof_3 (statistically significant bands)\n",
    "    bands_tof_3 = [\n",
    "        (0, 47),    \n",
    "        (50, 55)\n",
    "    ]\n",
    "    \n",
    "    for tof_num in [3]:  # Only process tof_3\n",
    "        for band_idx, (start, end) in enumerate(bands_tof_3, 1):\n",
    "            # Get columns for this band (inclusive range)\n",
    "            band_cols = [f\"tof_{tof_num}_v{p}\" for p in range(start, end + 1)]\n",
    "            band_data = seq_data[band_cols].values\n",
    "            band_data[band_data == -1] = np.nan\n",
    "            \n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                seq_data[f'tof_{tof_num}_band{band_idx}_mean'] = np.nanmean(band_data, axis=1)\n",
    "                seq_data[f'tof_{tof_num}_band{band_idx}_std'] = np.nanstd(band_data, axis=1)\n",
    "                seq_data[f'tof_{tof_num}_band{band_idx}_var'] = np.nanvar(band_data, axis=1)\n",
    "                seq_data[f'tof_{tof_num}_band{band_idx}_min'] = np.nanmin(band_data, axis=1)\n",
    "                seq_data[f'tof_{tof_num}_band{band_idx}_max'] = np.nanmax(band_data, axis=1)\n",
    "                \n",
    "    # ToF band statistics for tof_4 (statistically significant bands)\n",
    "    bands_tof_4 = [\n",
    "        (0, 3),    \n",
    "        (7, 9),\n",
    "        (15, 16),\n",
    "        (21, 23),\n",
    "        (28, 31),\n",
    "        (35, 39),\n",
    "        (43, 47),\n",
    "        (50, 55),\n",
    "        (58, 63)\n",
    "    ]\n",
    "    \n",
    "    for tof_num in [4]:  # Only process tof_4\n",
    "        for band_idx, (start, end) in enumerate(bands_tof_4, 1):\n",
    "            # Get columns for this band (inclusive range)\n",
    "            band_cols = [f\"tof_{tof_num}_v{p}\" for p in range(start, end + 1)]\n",
    "            band_data = seq_data[band_cols].values\n",
    "            band_data[band_data == -1] = np.nan\n",
    "            \n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                seq_data[f'tof_{tof_num}_band{band_idx}_mean'] = np.nanmean(band_data, axis=1)\n",
    "                seq_data[f'tof_{tof_num}_band{band_idx}_std'] = np.nanstd(band_data, axis=1)\n",
    "                seq_data[f'tof_{tof_num}_band{band_idx}_var'] = np.nanvar(band_data, axis=1)\n",
    "                seq_data[f'tof_{tof_num}_band{band_idx}_min'] = np.nanmin(band_data, axis=1)\n",
    "                seq_data[f'tof_{tof_num}_band{band_idx}_max'] = np.nanmax(band_data, axis=1)\n",
    "                \n",
    "    # ToF band statistics for tof_5 (statistically significant bands)\n",
    "    bands_tof_5 = [\n",
    "        (1, 7),    \n",
    "        (9, 15),\n",
    "        (18, 23),\n",
    "        (48, 49),\n",
    "        (56, 61)  \n",
    "    ]\n",
    "    \n",
    "    for tof_num in [5]:  # Only process tof_5\n",
    "        for band_idx, (start, end) in enumerate(bands_tof_5, 1):\n",
    "            # Get columns for this band (inclusive range)\n",
    "            band_cols = [f\"tof_{tof_num}_v{p}\" for p in range(start, end + 1)]\n",
    "            band_data = seq_data[band_cols].values\n",
    "            band_data[band_data == -1] = np.nan\n",
    "            \n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                seq_data[f'tof_{tof_num}_band{band_idx}_mean'] = np.nanmean(band_data, axis=1)\n",
    "                seq_data[f'tof_{tof_num}_band{band_idx}_std'] = np.nanstd(band_data, axis=1)\n",
    "                seq_data[f'tof_{tof_num}_band{band_idx}_var'] = np.nanvar(band_data, axis=1)\n",
    "                seq_data[f'tof_{tof_num}_band{band_idx}_min'] = np.nanmin(band_data, axis=1)\n",
    "                seq_data[f'tof_{tof_num}_band{band_idx}_max'] = np.nanmax(band_data, axis=1)            \n",
    "    \n",
    "    return seq_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5e84c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding gestures...\n",
      "Done\n",
      "Processing sequences with parallel computation...\n",
      "Using 8 cores for parallel processing\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    # Load data\n",
    "    df = pd.read_csv('input/train.csv', low_memory=False)\n",
    "    train_dem_df = pd.read_csv('input/train_demographics.csv', low_memory=False)\n",
    "    df = pd.merge(df, train_dem_df, on='subject', how='left')\n",
    "    \n",
    "    print(\"Encoding gestures...\")\n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    df['gesture_int'] = le.fit_transform(df['gesture'])\n",
    "    gesture_classes = le.classes_\n",
    "    np.save(model_output_path / \"gesture_classes.npy\", gesture_classes)\n",
    "    \n",
    "    print(\"Done\")\n",
    "    \n",
    "    print(\"Processing sequences with parallel computation...\")\n",
    "    \n",
    "    # Group by sequence\n",
    "    sequences = [group for _, group in df.groupby('sequence_id')]\n",
    "    \n",
    "    # Process in parallel\n",
    "    n_cores = multiprocessing.cpu_count()\n",
    "    print(f\"Using {n_cores} cores for parallel processing\")\n",
    "    \n",
    "    processed_sequences = Parallel(n_jobs=n_cores)(\n",
    "        delayed(process_sequence)(seq) for seq in sequences\n",
    "    )\n",
    "\n",
    "    # Combine processed sequences\n",
    "    df = pd.concat(processed_sequences, ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "processed_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20b25add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature testing\n",
    "def feature_engineering(df):   \n",
    "    df = df.copy()\n",
    "\n",
    "    # checks N/A before combining\n",
    "    def combine_or_na(a, b):\n",
    "        if 'N/A' in str(a) or 'N/A' in str(b):\n",
    "            return 'N/A'\n",
    "        return f\"{a}_{b}\"\n",
    "\n",
    "    # combine orientation and gesture\n",
    "    df['orientation_gesture'] = df.apply(lambda x: combine_or_na(x['orientation'], x['gesture']), axis=1).astype('category') \n",
    "        \n",
    "    # behavioural boolean columns\n",
    "    df['performs_gesture'] = df['behavior'].str.contains('Performs gesture', case=False, na=False)\n",
    "    df['move_hand_to_target'] = df['behavior'].str.contains('Moves hand to target location', case=False, na=False)\n",
    "    df['hand_at_target'] = df['behavior'].str.contains('Hand at target location', case=False, na=False)\n",
    "    df['relaxes_moves_hand_to_target'] = df['behavior'].str.contains('Relaxes and moves hand to target location', case=False, na=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "fe_train_df = feature_engineering(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dac805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(df):\n",
    "    # Fill categorical columns with 'N/A'\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns\n",
    "    df[cat_cols] = df[cat_cols].fillna(\"N/A\")\n",
    "    \n",
    "    # Fill all numerical columns with 0 (int8, int16, int32, int64, uint8, uint16, uint32, uint64, float16, float32, float64, complex64, complex128)\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns  \n",
    "    df[num_cols] = df[num_cols].replace([np.inf, -np.inf, '', None], np.nan).fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "fe_train_df  = fill_missing_values(fe_train_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7477a49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['row_id',\n",
       " 'sequence_type',\n",
       " 'sequence_id',\n",
       " 'subject',\n",
       " 'orientation',\n",
       " 'behavior',\n",
       " 'phase',\n",
       " 'gesture',\n",
       " 'orientation_gesture',\n",
       " 'performs_gesture',\n",
       " 'move_hand_to_target',\n",
       " 'hand_at_target',\n",
       " 'relaxes_moves_hand_to_target']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get categorical columns\n",
    "cat_features = fe_train_df.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed18cf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding Categorical Features: row_id, sequence_type, sequence_id, subject, orientation, behavior, phase, gesture, orientation_gesture, performs_gesture, move_hand_to_target, hand_at_target, relaxes_moves_hand_to_target, "
     ]
    }
   ],
   "source": [
    "# Dictionary to store all encoder mappings\n",
    "encoder_mappings = {}\n",
    "\n",
    "print(\"Label Encoding Categorical Features: \",end=\"\")\n",
    "for c in cat_features:\n",
    "    print(f\"{c}, \",end=\"\")\n",
    "    fe_train_df[c] =  fe_train_df[c].astype('category')\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    fe_train_df[c] = le.fit_transform(fe_train_df[c].astype(str))\n",
    "\n",
    "    # Save the mapping of numerical values to string labels\n",
    "    encoder_mappings[c] = {\n",
    "        i: label for i, label in enumerate(le.classes_)\n",
    "    }\n",
    "\n",
    "with open('new_categorical_encoder_mappings.json', 'w') as f:\n",
    "    json.dump(encoder_mappings, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8de72514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical distributions\n",
    "def plot_numerical_distributions(df, output_path, exclude_zeros=False):\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns\n",
    "    \n",
    "    # Create progress bar for numerical distributions\n",
    "    for col in tqdm(numerical_cols, desc=\"Creating distribution plots\"):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Filter data for plotting if exclude_zeros is True\n",
    "        if exclude_zeros:\n",
    "            plot_data = df[df[col] != 0][col]\n",
    "            title_suffix = \" (excluding zeros)\"\n",
    "        else:\n",
    "            plot_data = df[col]\n",
    "            title_suffix = \"\"\n",
    "        \n",
    "        # Skip if no data remains after filtering\n",
    "        if len(plot_data) == 0:\n",
    "            print(f\"Warning: No data remaining for {col} after excluding zeros\")\n",
    "            plt.close()\n",
    "            continue\n",
    "        \n",
    "        # Create subplot with histogram and kde\n",
    "        sns.histplot(data=plot_data, kde=True)\n",
    "        plt.title(f'Distribution of {col}{title_suffix}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        # Add statistical annotations based on filtered data\n",
    "        stats_text = f'Mean: {plot_data.mean():.2f}\\n'\n",
    "        stats_text += f'Median: {plot_data.median():.2f}\\n'\n",
    "        stats_text += f'Std: {plot_data.std():.2f}\\n'\n",
    "        stats_text += f'Count: {len(plot_data)}'\n",
    "        \n",
    "        # Add additional info if zeros were excluded\n",
    "        if exclude_zeros:\n",
    "            zero_count = (df[col] == 0).sum()\n",
    "            stats_text += f'\\nZeros excluded: {zero_count}'\n",
    "        \n",
    "        plt.text(0.95, 0.95, stats_text,\n",
    "                transform=plt.gca().transAxes,\n",
    "                verticalalignment='top',\n",
    "                horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Modify filename if zeros are excluded\n",
    "        filename_suffix = \"_no_zeros\" if exclude_zeros else \"\"\n",
    "        plt.savefig(f'{output_path}/distribution_{col}{filename_suffix}.png')\n",
    "        plt.close()\n",
    "\n",
    "# Exclude zeros from visualization\n",
    "# plot_numerical_distributions(train_df, plot_output_path, exclude_zeros=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2862cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical distributions\n",
    "def plot_categorical_distributions(df, output_path):\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    # Create progress bar for categorical distributions\n",
    "    for col in tqdm(categorical_cols, desc=\"Creating distribution plots\"):\n",
    "        # Skip columns with 100 or more unique values\n",
    "        if df[col].nunique() >= 100:\n",
    "            print(f\"Skipping {col}: too many unique values ({df[col].nunique()})\")\n",
    "            continue\n",
    "            \n",
    "        plt.figure(figsize=(18, 10))\n",
    "        \n",
    "        plot_data = df[col]\n",
    "        \n",
    "        # Skip if no data remains\n",
    "        if len(plot_data) == 0:\n",
    "            print(f\"Warning: No data remaining for {col}\")\n",
    "            plt.close()\n",
    "            continue\n",
    "        \n",
    "        # Create subplot with count plot\n",
    "        sns.countplot(data=plot_data.to_frame(), y=col, order=plot_data.value_counts().index)\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel('Count')\n",
    "        plt.ylabel(col)\n",
    "        \n",
    "        # Add statistical annotations\n",
    "        stats_text = f'Unique: {plot_data.nunique()}\\n'\n",
    "        stats_text += f'Top freq: {plot_data.value_counts().iloc[0] if len(plot_data.value_counts()) > 0 else 0}\\n'\n",
    "        stats_text += f'Count: {len(plot_data)}'\n",
    "        \n",
    "        plt.text(0.95, 0.10, stats_text,\n",
    "                transform=plt.gca().transAxes,\n",
    "                verticalalignment='top',\n",
    "                horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.savefig(f'{output_path}/distribution_{col}.png')\n",
    "        plt.close()\n",
    "\n",
    "# Plot categorical distributions\n",
    "# plot_categorical_distributions(train_df, plot_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d2457a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1 (True): 255817 records\n",
      "Group 2 (False): 319128 records\n",
      "Testing 406 numerical variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running t-tests and creating plots: 100%|██████████| 406/406 [1:23:07<00:00, 12.29s/it] \n"
     ]
    }
   ],
   "source": [
    "# Ttest analysis\n",
    "def perform_ttest_analysis(df, group_col, group1_value, group2_value, output_path):  \n",
    "    # Filter data for the two groups\n",
    "    group1_data = df[df[group_col] == group1_value].copy()\n",
    "    group2_data = df[df[group_col] == group2_value].copy()\n",
    "    \n",
    "    print(f\"Group 1 ({group1_value}): {len(group1_data)} records\")\n",
    "    print(f\"Group 2 ({group2_value}): {len(group2_data)} records\")\n",
    "    \n",
    "    # Get numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns.tolist()\n",
    "    \n",
    "    # Remove the group column if it's numerical\n",
    "    if group_col in numerical_cols:\n",
    "        numerical_cols.remove(group_col)\n",
    "    \n",
    "    print(f\"Testing {len(numerical_cols)} numerical variables\")\n",
    "    \n",
    "    # Initialize results list\n",
    "    results = []\n",
    "    \n",
    "    # Perform t-tests for each numerical column\n",
    "    for col in tqdm(numerical_cols, desc=\"Running t-tests and creating plots\"):\n",
    "        try:\n",
    "            # Get data for both groups for this column (remove NaN values)\n",
    "            group1_values = group1_data[col].dropna()\n",
    "            group2_values = group2_data[col].dropna()\n",
    "            \n",
    "            # Skip if insufficient data\n",
    "            if len(group1_values) < 2 or len(group2_values) < 2:\n",
    "                print(f\"Warning: Insufficient data for {col} (Group1: {len(group1_values)}, Group2: {len(group2_values)})\")\n",
    "                continue\n",
    "            \n",
    "            # Perform Levene's test for equal variances\n",
    "            levene_stat, levene_p = stats.levene(group1_values, group2_values)\n",
    "            equal_var = levene_p > 0.05\n",
    "            \n",
    "            # Perform independent t-test\n",
    "            t_stat, p_value = stats.ttest_ind(group1_values, group2_values, equal_var=equal_var)\n",
    "            \n",
    "            # Calculate effect size (Cohen's d)\n",
    "            if equal_var:\n",
    "                pooled_std = np.sqrt(((len(group1_values) - 1) * group1_values.var() + \n",
    "                                     (len(group2_values) - 1) * group2_values.var()) / \n",
    "                                    (len(group1_values) + len(group2_values) - 2))\n",
    "            else:\n",
    "                pooled_std = np.sqrt((group1_values.var() + group2_values.var()) / 2)\n",
    "            \n",
    "            cohens_d = (group1_values.mean() - group2_values.mean()) / pooled_std if pooled_std != 0 else 0\n",
    "            \n",
    "            # Calculate 95% confidence interval for the difference in means\n",
    "            diff_mean = group2_values.mean() - group1_values.mean()\n",
    "            se_diff = np.sqrt(group1_values.var()/len(group1_values) + group2_values.var()/len(group2_values))\n",
    "            t_critical = stats.t.ppf(0.975, len(group1_values) + len(group2_values) - 2)\n",
    "            ci_lower = diff_mean - t_critical * se_diff\n",
    "            ci_upper = diff_mean + t_critical * se_diff\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'variable': col,\n",
    "                'group1_mean': group1_values.mean(),\n",
    "                'group1_std': group1_values.std(),\n",
    "                'group1_median': group1_values.median(),\n",
    "                'group1_count': len(group1_values),\n",
    "                'group2_mean': group2_values.mean(),\n",
    "                'group2_std': group2_values.std(),\n",
    "                'group2_median': group2_values.median(),\n",
    "                'group2_count': len(group2_values),\n",
    "                'mean_difference': diff_mean,\n",
    "                'ci_lower': ci_lower,\n",
    "                'ci_upper': ci_upper,\n",
    "                't_statistic': t_stat,\n",
    "                'p_value': p_value,\n",
    "                'cohens_d': cohens_d,\n",
    "                'equal_variances': equal_var,\n",
    "                'levene_p_value': levene_p,\n",
    "                'significant_005': p_value < 0.05,\n",
    "                'significant_001': p_value < 0.01,\n",
    "                'significant_0001': p_value < 0.001\n",
    "            })\n",
    "            \n",
    "            # Create comparison plot\n",
    "            create_comparison_plot(group_col, group1_values, group2_values, col, \n",
    "                                 group1_value, group2_value, \n",
    "                                 t_stat, p_value, cohens_d, output_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {col}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    if len(results_df) > 0:\n",
    "        # Sort by p-value\n",
    "        results_df = results_df.sort_values('p_value').reset_index(drop=True)\n",
    "        \n",
    "        # Add interpretation columns\n",
    "        results_df['effect_size_interpretation'] = results_df['cohens_d'].abs().apply(interpret_cohens_d)\n",
    "        results_df['significance_level'] = results_df.apply(get_significance_level, axis=1)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def interpret_cohens_d(d):\n",
    "    \"\"\"Interpret Cohen's d effect size\"\"\"\n",
    "    d_abs = abs(d)\n",
    "    if d_abs < 0.2:\n",
    "        return \"negligible\"\n",
    "    elif d_abs < 0.5:\n",
    "        return \"small\"\n",
    "    elif d_abs < 0.8:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"large\"\n",
    "\n",
    "def get_significance_level(row):\n",
    "    \"\"\"Get significance level string\"\"\"\n",
    "    if row['p_value'] < 0.001:\n",
    "        return \"*** (p < 0.001)\"\n",
    "    elif row['p_value'] < 0.01:\n",
    "        return \"** (p < 0.01)\"\n",
    "    elif row['p_value'] < 0.05:\n",
    "        return \"* (p < 0.05)\"\n",
    "    else:\n",
    "        return \"Not significant\"\n",
    "\n",
    "def create_comparison_plot(group_col, group1_values, group2_values, col_name, \n",
    "                          group1_name, group2_name, t_stat, p_value, cohens_d, output_path):\n",
    "    \"\"\"Create comprehensive comparison plot for two groups\"\"\"\n",
    "    \n",
    "    # Set up the figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f'Statistical Comparison: {col_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Color scheme\n",
    "    color1 = '#ff7f7f'  # Light red for group 1\n",
    "    color2 = '#7fbfff'  # Light blue for group 2\n",
    "    \n",
    "    # 1. Histogram comparison (top left)\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    # Calculate bins for better visualization\n",
    "    all_values = np.concatenate([group1_values, group2_values])\n",
    "    bins = np.histogram_bin_edges(all_values, bins=30)\n",
    "    \n",
    "    ax1.hist(group1_values, bins=bins, alpha=0.7, label=f'{group1_name} (n={len(group1_values)})', \n",
    "             color=color1, density=True, edgecolor='black', linewidth=0.5)\n",
    "    ax1.hist(group2_values, bins=bins, alpha=0.7, label=f'{group2_name} (n={len(group2_values)})', \n",
    "             color=color2, density=True, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # Add mean lines\n",
    "    ax1.axvline(group1_values.mean(), color='darkred', linestyle='--', linewidth=2, \n",
    "                label=f'{group1_name} Mean: {group1_values.mean():.2f}')\n",
    "    ax1.axvline(group2_values.mean(), color='darkblue', linestyle='--', linewidth=2,\n",
    "                label=f'{group2_name} Mean: {group2_values.mean():.2f}')\n",
    "    \n",
    "    ax1.set_title('Distribution Comparison (Histogram)')\n",
    "    ax1.set_xlabel(col_name)\n",
    "    ax1.set_ylabel('Density')\n",
    "    ax1.legend(fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Box plot comparison (top right)\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    box_data = [group1_values, group2_values]\n",
    "    box_labels = [f'{group1_name}\\n(n={len(group1_values)})', f'{group2_name}\\n(n={len(group2_values)})']\n",
    "    \n",
    "    bp = ax2.boxplot(box_data, labels=box_labels, patch_artist=True, \n",
    "                     boxprops=dict(alpha=0.7), showfliers=True)\n",
    "    bp['boxes'][0].set_facecolor(color1)\n",
    "    bp['boxes'][1].set_facecolor(color2)\n",
    "    \n",
    "    ax2.set_title('Box Plot Comparison')\n",
    "    ax2.set_ylabel(col_name)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. KDE plot (bottom left)\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    try:\n",
    "        # Only plot KDE if we have enough data points\n",
    "        if len(group1_values) > 3:\n",
    "            sns.kdeplot(data=group1_values, label=f'{group1_name}', \n",
    "                       color='darkred', ax=ax3, linewidth=2)\n",
    "        if len(group2_values) > 3:\n",
    "            sns.kdeplot(data=group2_values, label=f'{group2_name}', \n",
    "                       color='darkblue', ax=ax3, linewidth=2)\n",
    "    except:\n",
    "        # Fallback to simple line plot if KDE fails\n",
    "        ax3.hist(group1_values, alpha=0.5, density=True, color=color1, label=group1_name)\n",
    "        ax3.hist(group2_values, alpha=0.5, density=True, color=color2, label=group2_name)\n",
    "    \n",
    "    ax3.set_title('Density Comparison (KDE)')\n",
    "    ax3.set_xlabel(col_name)\n",
    "    ax3.set_ylabel('Density')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Statistics summary (bottom right)\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Calculate additional statistics\n",
    "    effect_size_interp = interpret_cohens_d(cohens_d)\n",
    "    sig_level = get_significance_level({'p_value': p_value})\n",
    "    \n",
    "    stats_text = f'T-Test Results:\\n'\n",
    "    stats_text += f'━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n'\n",
    "    stats_text += f'T-statistic: {t_stat:.4f}\\n'\n",
    "    stats_text += f'P-value: {p_value:.2e}\\n'\n",
    "    stats_text += f'Cohen\\'s d: {cohens_d:.4f} ({effect_size_interp})\\n'\n",
    "    stats_text += f'Significance: {sig_level}\\n\\n'\n",
    "    \n",
    "    stats_text += f'{group1_name}:\\n'\n",
    "    stats_text += f'  Mean ± SD: {group1_values.mean():.2f} ± {group1_values.std():.2f}\\n'\n",
    "    stats_text += f'  Median: {group1_values.median():.2f}\\n'\n",
    "    stats_text += f'  Range: [{group1_values.min():.2f}, {group1_values.max():.2f}]\\n'\n",
    "    stats_text += f'  Count: {len(group1_values)}\\n\\n'\n",
    "    \n",
    "    stats_text += f'{group2_name}:\\n'\n",
    "    stats_text += f'  Mean ± SD: {group2_values.mean():.2f} ± {group2_values.std():.2f}\\n'\n",
    "    stats_text += f'  Median: {group2_values.median():.2f}\\n'\n",
    "    stats_text += f'  Range: [{group2_values.min():.2f}, {group2_values.max():.2f}]\\n'\n",
    "    stats_text += f'  Count: {len(group2_values)}\\n\\n'\n",
    "    \n",
    "    # Difference\n",
    "    mean_diff = group2_values.mean() - group1_values.mean()\n",
    "    stats_text += f'Mean Difference: {mean_diff:.2f}\\n'\n",
    "    stats_text += f'({group2_name} - {group1_name})\\n\\n'\n",
    "    \n",
    "    # Effect size interpretation\n",
    "    stats_text += f'Effect Size Interpretation:\\n'\n",
    "    if abs(cohens_d) < 0.2:\n",
    "        stats_text += f'Negligible practical difference'\n",
    "    elif abs(cohens_d) < 0.5:\n",
    "        stats_text += f'Small practical difference'\n",
    "    elif abs(cohens_d) < 0.8:\n",
    "        stats_text += f'Medium practical difference'\n",
    "    else:\n",
    "        stats_text += f'Large practical difference'\n",
    "    \n",
    "    ax4.text(0.05, 0.95, stats_text,\n",
    "             transform=ax4.transAxes,\n",
    "             verticalalignment='top',\n",
    "             horizontalalignment='left',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8),\n",
    "             fontsize=10, fontfamily='monospace')\n",
    "    ax4.set_xlim(0, 1)\n",
    "    ax4.set_ylim(0, 1)\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save with significance indicator in filename\n",
    "    if p_value < 0.001:\n",
    "        sig_suffix = \"_highly_significant\"\n",
    "    elif p_value < 0.05:\n",
    "        sig_suffix = \"_significant\"\n",
    "    else:\n",
    "        sig_suffix = \"_not_significant\"\n",
    "    \n",
    "    plt.savefig(f'{output_path}/{group_col}_{col_name}_ttest_{group1_name}_vs_{group2_name}_{sig_suffix}_{effect_size_interp}.png', \n",
    "                dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Perform the analysis\n",
    "#ttest_results = perform_ttest_analysis(\n",
    "#    df=fe_train_df, \n",
    "#    group_col='performs_gesture',\n",
    "#    group1_value=True,\n",
    "#    group2_value=False,\n",
    "#    output_path=plot_output_path\n",
    "#)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
