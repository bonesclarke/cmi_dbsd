{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-21T15:24:07.572808Z",
     "iopub.status.busy": "2025-07-21T15:24:07.572176Z",
     "iopub.status.idle": "2025-07-21T15:24:28.695623Z",
     "shell.execute_reply": "2025-07-21T15:24:28.694815Z",
     "shell.execute_reply.started": "2025-07-21T15:24:07.572784Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, json, joblib\n",
    "# Core data science libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings \n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence, to_categorical, pad_sequences\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, BatchNormalization, Activation, add, MaxPooling1D, Dropout,\n",
    "    Bidirectional, LSTM, GlobalAveragePooling1D, Dense, Multiply, Reshape,\n",
    "    Lambda, Concatenate, GRU, GaussianNoise, Add, GlobalMaxPooling1D,\n",
    "    MultiHeadAttention, LayerNormalization\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "import polars as pl\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T15:24:28.697542Z",
     "iopub.status.busy": "2025-07-21T15:24:28.696821Z",
     "iopub.status.idle": "2025-07-21T15:24:28.701944Z",
     "shell.execute_reply": "2025-07-21T15:24:28.701445Z",
     "shell.execute_reply.started": "2025-07-21T15:24:28.697521Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.experimental.numpy.random.seed(seed)\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T15:24:28.702834Z",
     "iopub.status.busy": "2025-07-21T15:24:28.702658Z",
     "iopub.status.idle": "2025-07-21T15:24:28.742226Z",
     "shell.execute_reply": "2025-07-21T15:24:28.741693Z",
     "shell.execute_reply.started": "2025-07-21T15:24:28.702812Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "train = True                    \n",
    "raw_dir = Path(\"input\")\n",
    "pretrained_dir = Path(\"best_model\") # replace with my trained weights\n",
    "output_dir = Path(\"best_model\")                                    \n",
    "batch_size = 64 \n",
    "pad_percentile = 90\n",
    "lr_init = 5e-4\n",
    "wd = 3e-3\n",
    "mixup_alpha = 0.4\n",
    "epochs = 120  \n",
    "patience = 50 \n",
    "n_splits = 6 \n",
    "\n",
    "print(\"Imports ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T15:24:28.744096Z",
     "iopub.status.busy": "2025-07-21T15:24:28.743853Z",
     "iopub.status.idle": "2025-07-21T15:24:28.760983Z",
     "shell.execute_reply": "2025-07-21T15:24:28.760491Z",
     "shell.execute_reply.started": "2025-07-21T15:24:28.744079Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if train:\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T15:24:28.761684Z",
     "iopub.status.busy": "2025-07-21T15:24:28.761466Z",
     "iopub.status.idle": "2025-07-21T15:24:28.772634Z",
     "shell.execute_reply": "2025-07-21T15:24:28.771939Z",
     "shell.execute_reply.started": "2025-07-21T15:24:28.761665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Tensor Manipulations\n",
    "def time_sum(x):\n",
    "    return K.sum(x, axis=1)\n",
    "\n",
    "def squeeze_last_axis(x):\n",
    "    return tf.squeeze(x, axis=-1)\n",
    "\n",
    "def expand_last_axis(x):\n",
    "    return tf.expand_dims(x, axis=-1)\n",
    "\n",
    "def se_block(x, reduction=8):\n",
    "    ch = x.shape[-1]\n",
    "    se = GlobalAveragePooling1D()(x)\n",
    "    se = Dense(ch // reduction, activation='relu')(se)\n",
    "    se = Dense(ch, activation='sigmoid')(se)\n",
    "    se = Reshape((1, ch))(se)\n",
    "    return Multiply()([x, se])\n",
    "\n",
    "# Residual CNN Block with SE\n",
    "def residual_se_cnn_block(x, filters, kernel_size, pool_size=2, drop=0.3, wd=1e-4):\n",
    "    shortcut = x\n",
    "    for _ in range(2):\n",
    "        x = Conv1D(filters, kernel_size, padding='same', use_bias=False,\n",
    "                   kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "    x = se_block(x)\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, 1, padding='same', use_bias=False,\n",
    "                          kernel_regularizer=l2(wd))(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    x = add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(pool_size)(x)\n",
    "    x = Dropout(drop)(x)\n",
    "    return x\n",
    "\n",
    "def attention_layer(inputs):\n",
    "    score = Dense(1, activation='tanh')(inputs)\n",
    "    score = Lambda(squeeze_last_axis)(score)\n",
    "    weights = Activation('softmax')(score)\n",
    "    weights = Lambda(expand_last_axis)(weights)\n",
    "    context = Multiply()([inputs, weights])\n",
    "    context = Lambda(time_sum)(context)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T15:24:28.773412Z",
     "iopub.status.busy": "2025-07-21T15:24:28.773239Z",
     "iopub.status.idle": "2025-07-21T15:24:28.792693Z",
     "shell.execute_reply": "2025-07-21T15:24:28.792194Z",
     "shell.execute_reply.started": "2025-07-21T15:24:28.773398Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Optimized physics calculations using vectorization\n",
    "def remove_gravity_from_acc_vectorized(acc_values, quat_values):\n",
    "    \"\"\"Vectorized gravity removal for better performance\"\"\"\n",
    "    num_samples = acc_values.shape[0]\n",
    "    linear_accel = acc_values.copy()\n",
    "    \n",
    "    # Filter valid quaternions\n",
    "    valid_mask = ~(np.any(np.isnan(quat_values), axis=1) | \n",
    "                   np.all(np.isclose(quat_values, 0), axis=1))\n",
    "    \n",
    "    if np.any(valid_mask):\n",
    "        # Process all valid quaternions at once\n",
    "        valid_quats = quat_values[valid_mask]\n",
    "        \n",
    "        # Batch rotation computation\n",
    "        try:\n",
    "            rotations = R.from_quat(valid_quats)\n",
    "            gravity_world = np.array([0, 0, 9.81])\n",
    "            \n",
    "            # Apply rotations in batch\n",
    "            gravity_sensor_frames = rotations.apply(gravity_world, inverse=True)\n",
    "            linear_accel[valid_mask] = acc_values[valid_mask] - gravity_sensor_frames\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    return linear_accel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T15:24:28.793662Z",
     "iopub.status.busy": "2025-07-21T15:24:28.793377Z",
     "iopub.status.idle": "2025-07-21T15:24:28.815994Z",
     "shell.execute_reply": "2025-07-21T15:24:28.815469Z",
     "shell.execute_reply.started": "2025-07-21T15:24:28.793643Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_angular_velocity_vectorized(quat_values, time_delta=1/200):\n",
    "    \"\"\"Vectorized angular velocity calculation\"\"\"\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_vel = np.zeros((num_samples, 3))\n",
    "    \n",
    "    if num_samples < 2:\n",
    "        return angular_vel\n",
    "        \n",
    "    # Process in chunks for memory efficiency\n",
    "    chunk_size = 1000\n",
    "    for start in range(0, num_samples - 1, chunk_size):\n",
    "        end = min(start + chunk_size, num_samples - 1)\n",
    "        \n",
    "        q_t = quat_values[start:end]\n",
    "        q_t_plus_dt = quat_values[start+1:end+1]\n",
    "        \n",
    "        # Find valid pairs\n",
    "        valid_mask = ~(np.any(np.isnan(q_t), axis=1) | \n",
    "                      np.all(np.isclose(q_t, 0), axis=1) |\n",
    "                      np.any(np.isnan(q_t_plus_dt), axis=1) | \n",
    "                      np.all(np.isclose(q_t_plus_dt, 0), axis=1))\n",
    "        \n",
    "        if np.any(valid_mask):\n",
    "            try:\n",
    "                valid_indices = np.where(valid_mask)[0]\n",
    "                rot_t = R.from_quat(q_t[valid_mask])\n",
    "                rot_t_plus_dt = R.from_quat(q_t_plus_dt[valid_mask])\n",
    "                \n",
    "                # Batch computation\n",
    "                delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "                angular_vel[start + valid_indices] = delta_rot.as_rotvec() / time_delta\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "    return angular_vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T15:24:28.817001Z",
     "iopub.status.busy": "2025-07-21T15:24:28.816802Z",
     "iopub.status.idle": "2025-07-21T15:24:28.837677Z",
     "shell.execute_reply": "2025-07-21T15:24:28.837069Z",
     "shell.execute_reply.started": "2025-07-21T15:24:28.816986Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Simplified MixUp generator\n",
    "class FastMixupGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size, alpha=0.4):\n",
    "        self.X, self.y = X, y\n",
    "        self.batch = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.indices = np.arange(len(X))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch))\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        idx = self.indices[i*self.batch:(i+1)*self.batch]\n",
    "        Xb, yb = self.X[idx], self.y[idx]\n",
    "        \n",
    "        # Simple mixup\n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        perm = np.random.permutation(len(Xb))\n",
    "        X_mix = lam * Xb + (1-lam) * Xb[perm]\n",
    "        y_mix = lam * yb + (1-lam) * yb[perm]\n",
    "        \n",
    "        return X_mix.astype('float32'), y_mix.astype('float32')\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T15:24:28.838459Z",
     "iopub.status.busy": "2025-07-21T15:24:28.838251Z",
     "iopub.status.idle": "2025-07-21T15:24:28.858641Z",
     "shell.execute_reply": "2025-07-21T15:24:28.858068Z",
     "shell.execute_reply.started": "2025-07-21T15:24:28.838445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Simplified model architecture (slightly reduced complexity)\n",
    "def build_two_branch_model_optimized(pad_len, imu_dim, tof_dim, n_classes, wd=1e-4):\n",
    "    inp = Input(shape=(pad_len, imu_dim+tof_dim))\n",
    "    imu = Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "    tof = Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "    \n",
    "    # IMU branch - simplified\n",
    "    x1 = residual_se_cnn_block(imu, 64, 5, drop=0.1, wd=wd)\n",
    "    x1 = residual_se_cnn_block(x1, 128, 5, drop=0.1, wd=wd)\n",
    "    \n",
    "    # TOF branch - simplified\n",
    "    x2 = Conv1D(128, 1, padding='same', use_bias=False, kernel_regularizer=l2(wd))(tof)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Activation('relu')(x2)\n",
    "    x2 = residual_se_cnn_block(x2, 192, 3, drop=0.2, wd=wd)\n",
    "    x2 = residual_se_cnn_block(x2, 256, 3, drop=0.2, wd=wd)\n",
    "    \n",
    "    # Simple concatenation\n",
    "    merged = Concatenate()([x1, x2])\n",
    "    \n",
    "    # Single RNN layer instead of multiple\n",
    "    x = Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(wd),\n",
    "                         dropout=0.2, recurrent_dropout=0.2))(merged)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Attention\n",
    "    x_att = attention_layer(x)\n",
    "    x_pool = GlobalAveragePooling1D()(x)\n",
    "    x = Concatenate()([x_att, x_pool])\n",
    "    \n",
    "    # Simplified classifier\n",
    "    x = Dense(256, use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = Dense(128, use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Output layer with float32 for stability with mixed precision\n",
    "    x = Dense(n_classes, kernel_regularizer=l2(wd), dtype='float32')(x)\n",
    "    out = Activation('softmax', dtype='float32')(x)\n",
    "    \n",
    "    return Model(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T15:24:28.861076Z",
     "iopub.status.busy": "2025-07-21T15:24:28.860728Z",
     "iopub.status.idle": "2025-07-21T15:24:28.885249Z",
     "shell.execute_reply": "2025-07-21T15:24:28.884726Z",
     "shell.execute_reply.started": "2025-07-21T15:24:28.861062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Parallel processing for sequence features\n",
    "def process_sequence(seq_data):\n",
    "    \"\"\"Process a single sequence with all feature engineering\"\"\"\n",
    "    seq_data = seq_data.copy()\n",
    "    \n",
    "    # Dictionary to collect all new columns\n",
    "    new_columns = {}\n",
    "    \n",
    "    # Get numpy arrays for faster processing\n",
    "    acc_values = seq_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    quat_values = seq_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "\n",
    "    try:\n",
    "        rotations = R.from_quat(quat_values)\n",
    "        euler_angles = rotations.as_euler('xyz', degrees=True)\n",
    "        new_columns['euler_x'] = euler_angles[:, 0]\n",
    "        new_columns['euler_y'] = euler_angles[:, 1]\n",
    "        new_columns['euler_z'] = euler_angles[:, 2]\n",
    "    except:\n",
    "        new_columns['euler_x'] = np.zeros(len(seq_data))\n",
    "        new_columns['euler_y'] = np.zeros(len(seq_data))\n",
    "        new_columns['euler_z'] = np.zeros(len(seq_data))\n",
    "    \n",
    "    # Linear acceleration\n",
    "    linear_accel = remove_gravity_from_acc_vectorized(acc_values, quat_values)\n",
    "    new_columns['linear_acc_x'] = linear_accel[:, 0]\n",
    "    new_columns['linear_acc_y'] = linear_accel[:, 1]\n",
    "    new_columns['linear_acc_z'] = linear_accel[:, 2]\n",
    "\n",
    "    # Acceleration features focusing on z and y axes\n",
    "    new_columns['acc_yz_mag'] = np.sqrt(seq_data['acc_y']**2 + seq_data['acc_z']**2)\n",
    "    new_columns['acc_y_z_ratio'] = seq_data['acc_y'] / (seq_data['acc_z'] + 1e-8)\n",
    "    \n",
    "    # Magnitudes\n",
    "    new_columns['acc_mag'] = np.linalg.norm(acc_values, axis=1)\n",
    "    new_columns['linear_acc_mag'] = np.linalg.norm(linear_accel, axis=1)\n",
    "    \n",
    "    # Jerk (simplified)\n",
    "    new_columns['linear_acc_mag_jerk'] = np.gradient(new_columns['linear_acc_mag']) * 200\n",
    "    \n",
    "    # Angular velocity\n",
    "    angular_vel = calculate_angular_velocity_vectorized(quat_values)\n",
    "    new_columns['angular_vel_x'] = angular_vel[:, 0]\n",
    "    new_columns['angular_vel_y'] = angular_vel[:, 1]\n",
    "    new_columns['angular_vel_z'] = angular_vel[:, 2]\n",
    "    \n",
    "    # ToF aggregations (vectorized)\n",
    "    for i in range(1, 6):\n",
    "        pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "        tof_data = seq_data[pixel_cols].values\n",
    "        tof_data[tof_data == -1] = np.nan\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            new_columns[f'tof_{i}_mean'] = np.nanmean(tof_data, axis=1)\n",
    "            new_columns[f'tof_{i}_std'] = np.nanstd(tof_data, axis=1)\n",
    "            new_columns[f'tof_{i}_min'] = np.nanmin(tof_data, axis=1)\n",
    "            new_columns[f'tof_{i}_max'] = np.nanmax(tof_data, axis=1)\n",
    "\n",
    "    # Add rotation-specific features for rot_z and rot_w\n",
    "    new_columns['rot_z_w_product'] = seq_data['rot_z'] * seq_data['rot_w']\n",
    "    new_columns['rot_z_w_ratio'] = seq_data['rot_z'] / (seq_data['rot_w'] + 1e-8)\n",
    "\n",
    "    # Rolling statistics for important features (window of 10 samples = 50ms)\n",
    "    for col in ['rot_z', 'rot_w', 'acc_z', 'acc_y', 'thm_2']:\n",
    "        new_columns[f'{col}_rolling_mean'] = seq_data[col].rolling(10, center=True).mean()\n",
    "        new_columns[f'{col}_rolling_std'] = seq_data[col].rolling(10, center=True).std()\n",
    "        new_columns[f'{col}_diff'] = seq_data[col].diff()\n",
    "    \n",
    "    # Thermopile 2 specific features\n",
    "    thm_2_mean = seq_data['thm_2'].mean()\n",
    "    thm_2_std = seq_data['thm_2'].std()\n",
    "    new_columns['thm_2_normalized'] = (seq_data['thm_2'] - thm_2_mean) / (thm_2_std + 1e-8)\n",
    "    new_columns['thm_2_delta_from_mean'] = seq_data['thm_2'] - seq_data[['thm_1', 'thm_2', 'thm_3', 'thm_4', 'thm_5']].mean(axis=1)\n",
    "\n",
    "    # ToF band statistics for tof_1 and tof_2 (statistically significant bands)\n",
    "    bands_tof_1_2 = [\n",
    "        (3, 7),    # Band 1: v[3-7]\n",
    "        (11, 15),  # Band 2: v[11-15]\n",
    "        (19, 23),  # Band 3: v[19-23]\n",
    "        (27, 31),  # Band 4: v[27-31]\n",
    "        (35, 39),  # Band 5: v[35-39]\n",
    "        (43, 47),  # Band 6: v[43-47]\n",
    "        (51, 55),  # Band 7: v[51-55]\n",
    "        (59, 63),  # Band 8: v[59-63]\n",
    "    ]\n",
    "    \n",
    "    for tof_num in [1, 2]:  # Only process tof_1 and tof_2\n",
    "        for band_idx, (start, end) in enumerate(bands_tof_1_2, 1):\n",
    "            # Get columns for this band (inclusive range)\n",
    "            band_cols = [f\"tof_{tof_num}_v{p}\" for p in range(start, end + 1)]\n",
    "            band_data = seq_data[band_cols].values\n",
    "            band_data[band_data == -1] = np.nan\n",
    "            \n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                new_columns[f'tof_{tof_num}_band{band_idx}_mean'] = np.nanmean(band_data, axis=1)\n",
    "                new_columns[f'tof_{tof_num}_band{band_idx}_std'] = np.nanstd(band_data, axis=1)\n",
    "                new_columns[f'tof_{tof_num}_band{band_idx}_var'] = np.nanvar(band_data, axis=1)\n",
    "                new_columns[f'tof_{tof_num}_band{band_idx}_min'] = np.nanmin(band_data, axis=1)\n",
    "                new_columns[f'tof_{tof_num}_band{band_idx}_max'] = np.nanmax(band_data, axis=1)\n",
    "                \n",
    "    # ToF band statistics for tof_3 (statistically significant bands)\n",
    "    bands_tof_3 = [\n",
    "        (0, 47),    \n",
    "        (50, 55)\n",
    "    ]\n",
    "    \n",
    "    for tof_num in [3]:  # Only process tof_3\n",
    "        for band_idx, (start, end) in enumerate(bands_tof_3, 1):\n",
    "            # Get columns for this band (inclusive range)\n",
    "            band_cols = [f\"tof_{tof_num}_v{p}\" for p in range(start, end + 1)]\n",
    "            band_data = seq_data[band_cols].values\n",
    "            band_data[band_data == -1] = np.nan\n",
    "            \n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                new_columns[f'tof_{tof_num}_band{band_idx}_mean'] = np.nanmean(band_data, axis=1)\n",
    "                new_columns[f'tof_{tof_num}_band{band_idx}_std'] = np.nanstd(band_data, axis=1)\n",
    "                new_columns[f'tof_{tof_num}_band{band_idx}_var'] = np.nanvar(band_data, axis=1)\n",
    "                new_columns[f'tof_{tof_num}_band{band_idx}_min'] = np.nanmin(band_data, axis=1)\n",
    "                new_columns[f'tof_{tof_num}_band{band_idx}_max'] = np.nanmax(band_data, axis=1)\n",
    "                \n",
    "    # ToF band statistics for tof_4 (statistically significant bands)\n",
    "    bands_tof_4 = [\n",
    "        (0, 3),    \n",
    "        (7, 9),\n",
    "        (15, 16),\n",
    "        (21, 23),\n",
    "        (28, 31),\n",
    "        (35, 39),\n",
    "        (43, 47),\n",
    "        (50, 55),\n",
    "        (58, 63)\n",
    "    ]\n",
    "    \n",
    "    for tof_num in [4]:  # Only process tof_4\n",
    "        for band_idx, (start, end) in enumerate(bands_tof_4, 1):\n",
    "            # Get columns for this band (inclusive range)\n",
    "            band_cols = [f\"tof_{tof_num}_v{p}\" for p in range(start, end + 1)]\n",
    "            band_data = seq_data[band_cols].values\n",
    "            band_data[band_data == -1] = np.nan\n",
    "            \n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                new_columns[f'tof_{tof_num}_band{band_idx}_mean'] = np.nanmean(band_data, axis=1)\n",
    "                new_columns[f'tof_{tof_num}_band{band_idx}_std'] = np.nanstd(band_data, axis=1)\n",
    "                new_columns[f'tof_{tof_num}_band{band_idx}_var'] = np.nanvar(band_data, axis=1)\n",
    "                new_columns[f'tof_{tof_num}_band{band_idx}_min'] = np.nanmin(band_data, axis=1)\n",
    "                new_columns[f'tof_{tof_num}_band{band_idx}_max'] = np.nanmax(band_data, axis=1)\n",
    "                \n",
    "    # ToF band statistics for tof_5 (statistically significant bands)\n",
    "    bands_tof_5 = [\n",
    "        (1, 7),    \n",
    "        (9, 15),\n",
    "        (18, 23),\n",
    "        (48, 49),\n",
    "        (56, 61)  \n",
    "    ]\n",
    "    \n",
    "    for tof_num in [5]:  # Only process tof_5\n",
    "        for band_idx, (start, end) in enumerate(bands_tof_5, 1):\n",
    "            # Get columns for this band (inclusive range)\n",
    "            band_cols = [f\"tof_{tof_num}_v{p}\" for p in range(start, end + 1)]\n",
    "            band_data = seq_data[band_cols].values\n",
    "            band_data[band_data == -1] = np.nan\n",
    "            \n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                new_columns[f'tof_{tof_num}_band{band_idx}_mean'] = np.nanmean(band_data, axis=1)\n",
    "                new_columns[f'tof_{tof_num}_band{band_idx}_std'] = np.nanstd(band_data, axis=1)\n",
    "                new_columns[f'tof_{tof_num}_band{band_idx}_var'] = np.nanvar(band_data, axis=1)\n",
    "                new_columns[f'tof_{tof_num}_band{band_idx}_min'] = np.nanmin(band_data, axis=1)\n",
    "                new_columns[f'tof_{tof_num}_band{band_idx}_max'] = np.nanmax(band_data, axis=1) \n",
    "    \n",
    "    # Create DataFrame from new columns and concatenate with original\n",
    "    new_columns_df = pd.DataFrame(new_columns, index=seq_data.index)\n",
    "    result_df = pd.concat([seq_data, new_columns_df], axis=1)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T15:24:28.886155Z",
     "iopub.status.busy": "2025-07-21T15:24:28.885934Z",
     "iopub.status.idle": "2025-07-21T15:24:28.909875Z",
     "shell.execute_reply": "2025-07-21T15:24:28.909366Z",
     "shell.execute_reply.started": "2025-07-21T15:24:28.886130Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prediction function for individual sequences\n",
    "def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    # Convert to pandas for compatibility\n",
    "    df_seq = sequence.to_pandas()\n",
    "    \n",
    "    # Get numpy arrays for faster processing\n",
    "    acc_values = df_seq[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    quat_values = df_seq[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    \n",
    "    # Euler angles\n",
    "    try:\n",
    "        rotations = R.from_quat(quat_values)\n",
    "        euler_angles = rotations.as_euler('xyz', degrees=True)\n",
    "        df_seq['euler_x'] = euler_angles[:, 0]\n",
    "        df_seq['euler_y'] = euler_angles[:, 1]\n",
    "        df_seq['euler_z'] = euler_angles[:, 2]\n",
    "    except:\n",
    "        df_seq['euler_x'] = np.zeros(len(df_seq))\n",
    "        df_seq['euler_y'] = np.zeros(len(df_seq))\n",
    "        df_seq['euler_z'] = np.zeros(len(df_seq))\n",
    "    \n",
    "    # Linear acceleration\n",
    "    linear_accel = remove_gravity_from_acc_vectorized(acc_values, quat_values)\n",
    "    df_seq['linear_acc_x'] = linear_accel[:, 0]\n",
    "    df_seq['linear_acc_y'] = linear_accel[:, 1]\n",
    "    df_seq['linear_acc_z'] = linear_accel[:, 2]\n",
    "    \n",
    "    # Acceleration features focusing on z and y axes\n",
    "    df_seq['acc_yz_mag'] = np.sqrt(df_seq['acc_y']**2 + df_seq['acc_z']**2)\n",
    "    df_seq['acc_y_z_ratio'] = df_seq['acc_y'] / (df_seq['acc_z'] + 1e-8)\n",
    "    \n",
    "    # Magnitudes\n",
    "    df_seq['acc_mag'] = np.linalg.norm(acc_values, axis=1)\n",
    "    df_seq['linear_acc_mag'] = np.linalg.norm(linear_accel, axis=1)\n",
    "    \n",
    "    # Jerk (simplified)\n",
    "    df_seq['linear_acc_mag_jerk'] = np.gradient(df_seq['linear_acc_mag']) * 200\n",
    "    \n",
    "    # Angular velocity\n",
    "    angular_vel = calculate_angular_velocity_vectorized(quat_values)\n",
    "    df_seq['angular_vel_x'] = angular_vel[:, 0]\n",
    "    df_seq['angular_vel_y'] = angular_vel[:, 1]\n",
    "    df_seq['angular_vel_z'] = angular_vel[:, 2]\n",
    "    \n",
    "    # ToF aggregations (vectorized)\n",
    "    for i in range(1, 6):\n",
    "        pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "        tof_data = df_seq[pixel_cols].values\n",
    "        tof_data[tof_data == -1] = np.nan\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            df_seq[f'tof_{i}_mean'] = np.nanmean(tof_data, axis=1)\n",
    "            df_seq[f'tof_{i}_std'] = np.nanstd(tof_data, axis=1)\n",
    "            df_seq[f'tof_{i}_min'] = np.nanmin(tof_data, axis=1)\n",
    "            df_seq[f'tof_{i}_max'] = np.nanmax(tof_data, axis=1)\n",
    "    \n",
    "    # Add rotation-specific features for rot_z and rot_w\n",
    "    df_seq['rot_z_w_product'] = df_seq['rot_z'] * df_seq['rot_w']\n",
    "    df_seq['rot_z_w_ratio'] = df_seq['rot_z'] / (df_seq['rot_w'] + 1e-8)\n",
    "    \n",
    "    # Rolling statistics for important features (window of 10 samples = 50ms)\n",
    "    for col in ['rot_z', 'rot_w', 'acc_z', 'acc_y', 'thm_2']:\n",
    "        df_seq[f'{col}_rolling_mean'] = df_seq[col].rolling(10, center=True).mean()\n",
    "        df_seq[f'{col}_rolling_std'] = df_seq[col].rolling(10, center=True).std()\n",
    "        df_seq[f'{col}_diff'] = df_seq[col].diff()\n",
    "    \n",
    "    # Thermopile 2 specific features\n",
    "    thm_2_mean = df_seq['thm_2'].mean()\n",
    "    thm_2_std = df_seq['thm_2'].std()\n",
    "    df_seq['thm_2_normalized'] = (df_seq['thm_2'] - thm_2_mean) / (thm_2_std + 1e-8)\n",
    "    df_seq['thm_2_delta_from_mean'] = df_seq['thm_2'] - df_seq[['thm_1', 'thm_2', 'thm_3', 'thm_4', 'thm_5']].mean(axis=1)\n",
    "    \n",
    "    # ToF band statistics for tof_1 and tof_2 (statistically significant bands)\n",
    "    bands_tof_1_2 = [\n",
    "        (3, 7),    # Band 1: v[3-7]\n",
    "        (11, 15),  # Band 2: v[11-15]\n",
    "        (19, 23),  # Band 3: v[19-23]\n",
    "        (27, 31),  # Band 4: v[27-31]\n",
    "        (35, 39),  # Band 5: v[35-39]\n",
    "        (43, 47),  # Band 6: v[43-47]\n",
    "        (51, 55),  # Band 7: v[51-55]\n",
    "        (59, 63),  # Band 8: v[59-63]\n",
    "    ]\n",
    "    \n",
    "    for tof_num in [1, 2]:  # Only process tof_1 and tof_2\n",
    "        for band_idx, (start, end) in enumerate(bands_tof_1_2, 1):\n",
    "            # Get columns for this band (inclusive range)\n",
    "            band_cols = [f\"tof_{tof_num}_v{p}\" for p in range(start, end + 1)]\n",
    "            band_data = df_seq[band_cols].values\n",
    "            band_data[band_data == -1] = np.nan\n",
    "            \n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                df_seq[f'tof_{tof_num}_band{band_idx}_mean'] = np.nanmean(band_data, axis=1)\n",
    "                df_seq[f'tof_{tof_num}_band{band_idx}_std'] = np.nanstd(band_data, axis=1)\n",
    "                df_seq[f'tof_{tof_num}_band{band_idx}_var'] = np.nanvar(band_data, axis=1)\n",
    "                df_seq[f'tof_{tof_num}_band{band_idx}_min'] = np.nanmin(band_data, axis=1)\n",
    "                df_seq[f'tof_{tof_num}_band{band_idx}_max'] = np.nanmax(band_data, axis=1)\n",
    "    \n",
    "    # ToF band statistics for tof_3 (statistically significant bands)\n",
    "    bands_tof_3 = [\n",
    "        (0, 47),    \n",
    "        (50, 55)\n",
    "    ]\n",
    "    \n",
    "    for tof_num in [3]:  # Only process tof_3\n",
    "        for band_idx, (start, end) in enumerate(bands_tof_3, 1):\n",
    "            # Get columns for this band (inclusive range)\n",
    "            band_cols = [f\"tof_{tof_num}_v{p}\" for p in range(start, end + 1)]\n",
    "            band_data = df_seq[band_cols].values\n",
    "            band_data[band_data == -1] = np.nan\n",
    "            \n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                df_seq[f'tof_{tof_num}_band{band_idx}_mean'] = np.nanmean(band_data, axis=1)\n",
    "                df_seq[f'tof_{tof_num}_band{band_idx}_std'] = np.nanstd(band_data, axis=1)\n",
    "                df_seq[f'tof_{tof_num}_band{band_idx}_var'] = np.nanvar(band_data, axis=1)\n",
    "                df_seq[f'tof_{tof_num}_band{band_idx}_min'] = np.nanmin(band_data, axis=1)\n",
    "                df_seq[f'tof_{tof_num}_band{band_idx}_max'] = np.nanmax(band_data, axis=1)\n",
    "    \n",
    "    # ToF band statistics for tof_4 (statistically significant bands)\n",
    "    bands_tof_4 = [\n",
    "        (0, 3),    \n",
    "        (7, 9),\n",
    "        (15, 16),\n",
    "        (21, 23),\n",
    "        (28, 31),\n",
    "        (35, 39),\n",
    "        (43, 47),\n",
    "        (50, 55),\n",
    "        (58, 63)\n",
    "    ]\n",
    "    \n",
    "    for tof_num in [4]:  # Only process tof_4\n",
    "        for band_idx, (start, end) in enumerate(bands_tof_4, 1):\n",
    "            # Get columns for this band (inclusive range)\n",
    "            band_cols = [f\"tof_{tof_num}_v{p}\" for p in range(start, end + 1)]\n",
    "            band_data = df_seq[band_cols].values\n",
    "            band_data[band_data == -1] = np.nan\n",
    "            \n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                df_seq[f'tof_{tof_num}_band{band_idx}_mean'] = np.nanmean(band_data, axis=1)\n",
    "                df_seq[f'tof_{tof_num}_band{band_idx}_std'] = np.nanstd(band_data, axis=1)\n",
    "                df_seq[f'tof_{tof_num}_band{band_idx}_var'] = np.nanvar(band_data, axis=1)\n",
    "                df_seq[f'tof_{tof_num}_band{band_idx}_min'] = np.nanmin(band_data, axis=1)\n",
    "                df_seq[f'tof_{tof_num}_band{band_idx}_max'] = np.nanmax(band_data, axis=1)\n",
    "    \n",
    "    # ToF band statistics for tof_5 (statistically significant bands)\n",
    "    bands_tof_5 = [\n",
    "        (1, 7),    \n",
    "        (9, 15),\n",
    "        (18, 23),\n",
    "        (48, 49),\n",
    "        (56, 61)  \n",
    "    ]\n",
    "    \n",
    "    for tof_num in [5]:  # Only process tof_5\n",
    "        for band_idx, (start, end) in enumerate(bands_tof_5, 1):\n",
    "            # Get columns for this band (inclusive range)\n",
    "            band_cols = [f\"tof_{tof_num}_v{p}\" for p in range(start, end + 1)]\n",
    "            band_data = df_seq[band_cols].values\n",
    "            band_data[band_data == -1] = np.nan\n",
    "            \n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                df_seq[f'tof_{tof_num}_band{band_idx}_mean'] = np.nanmean(band_data, axis=1)\n",
    "                df_seq[f'tof_{tof_num}_band{band_idx}_std'] = np.nanstd(band_data, axis=1)\n",
    "                df_seq[f'tof_{tof_num}_band{band_idx}_var'] = np.nanvar(band_data, axis=1)\n",
    "                df_seq[f'tof_{tof_num}_band{band_idx}_min'] = np.nanmin(band_data, axis=1)\n",
    "                df_seq[f'tof_{tof_num}_band{band_idx}_max'] = np.nanmax(band_data, axis=1)\n",
    "    \n",
    "    # Extract features and scale\n",
    "    mat_unscaled = df_seq[final_feature_cols].fillna(0).values.astype('float32')\n",
    "    mat_scaled = scaler.transform(mat_unscaled)\n",
    "    \n",
    "    # Pad sequence\n",
    "    pad_input = pad_sequences([mat_scaled], maxlen=pad_len, padding='post', \n",
    "                             truncating='post', dtype='float32')\n",
    "    \n",
    "    # Ensemble prediction\n",
    "    all_preds = []\n",
    "    for model in models:\n",
    "        pred = model.predict(pad_input, verbose=0)[0]\n",
    "        all_preds.append(pred)\n",
    "    \n",
    "    # Average predictions\n",
    "    avg_pred = np.mean(all_preds, axis=0)\n",
    "    predicted_class_idx = avg_pred.argmax()\n",
    "    \n",
    "    # Return gesture class name\n",
    "    return gesture_classes[predicted_class_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Separate Data Load Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T15:24:28.910677Z",
     "iopub.status.busy": "2025-07-21T15:24:28.910456Z",
     "iopub.status.idle": "2025-07-21T15:24:59.701930Z",
     "shell.execute_reply": "2025-07-21T15:24:59.701302Z",
     "shell.execute_reply.started": "2025-07-21T15:24:28.910661Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(raw_dir / \"train.csv\")\n",
    "train_dem_df = pd.read_csv(raw_dir / \"train_demographics.csv\")\n",
    "train_df = pd.merge(train_df, train_dem_df, on='subject', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T15:24:59.703363Z",
     "iopub.status.busy": "2025-07-21T15:24:59.702661Z",
     "iopub.status.idle": "2025-07-21T15:27:04.283957Z",
     "shell.execute_reply": "2025-07-21T15:27:04.283318Z",
     "shell.execute_reply.started": "2025-07-21T15:24:59.703337Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_data(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"Processing sequences with parallel computation...\")\n",
    "    \n",
    "    # Group by sequence\n",
    "    sequences = [group for _, group in df.groupby('sequence_id')]\n",
    "    \n",
    "    # Process in parallel\n",
    "    n_cores = multiprocessing.cpu_count()\n",
    "    print(f\"Using {n_cores} cores for parallel processing\")\n",
    "    \n",
    "    processed_sequences = Parallel(n_jobs=n_cores)(\n",
    "        delayed(process_sequence)(seq) for seq in sequences\n",
    "    )\n",
    "\n",
    "    # Combine processed sequences\n",
    "    df = pd.concat(processed_sequences, ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "processed_df = load_data(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T15:27:04.285481Z",
     "iopub.status.busy": "2025-07-21T15:27:04.285189Z",
     "iopub.status.idle": "2025-07-21T15:27:31.506784Z",
     "shell.execute_reply": "2025-07-21T15:27:31.506220Z",
     "shell.execute_reply.started": "2025-07-21T15:27:04.285453Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering(df):   \n",
    "    df = df.copy()\n",
    "\n",
    "    # checks N/A before combining\n",
    "    def combine_or_na(a, b):\n",
    "        if 'N/A' in str(a) or 'N/A' in str(b):\n",
    "            return 'N/A'\n",
    "        return f\"{a}_{b}\"\n",
    "\n",
    "    # combine orientation and gesture\n",
    "    df['orientation_gesture'] = df.apply(lambda x: combine_or_na(x['orientation'], x['gesture']), axis=1).astype('category') \n",
    "        \n",
    "    # behavioural boolean columns\n",
    "    df['performs_gesture'] = df['behavior'].str.contains('Performs gesture', case=False, na=False)\n",
    "    df['move_hand_to_target'] = df['behavior'].str.contains('Moves hand to target location', case=False, na=False)\n",
    "    df['hand_at_target'] = df['behavior'].str.contains('Hand at target location', case=False, na=False)\n",
    "    df['relaxes_moves_hand_to_target'] = df['behavior'].str.contains('Relaxes and moves hand to target location', case=False, na=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "fe_train_df = feature_engineering(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T15:27:31.507741Z",
     "iopub.status.busy": "2025-07-21T15:27:31.507534Z",
     "iopub.status.idle": "2025-07-21T15:27:40.372427Z",
     "shell.execute_reply": "2025-07-21T15:27:40.371765Z",
     "shell.execute_reply.started": "2025-07-21T15:27:31.507724Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fill_missing_values(df):\n",
    "    # Fill categorical columns with 'N/A'\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns\n",
    "    df[cat_cols] = df[cat_cols].fillna(\"N/A\")\n",
    "    \n",
    "    # Fill all numerical columns with 0 (int8, int16, int32, int64, uint8, uint16, uint32, uint64, float16, float32, float64, complex64, complex128)\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns  \n",
    "    df[num_cols] = df[num_cols].replace([np.inf, -np.inf, '', None], np.nan).fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "fe_train_df = fill_missing_values(fe_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T15:27:40.373542Z",
     "iopub.status.busy": "2025-07-21T15:27:40.373286Z",
     "iopub.status.idle": "2025-07-21T15:27:40.488995Z",
     "shell.execute_reply": "2025-07-21T15:27:40.488202Z",
     "shell.execute_reply.started": "2025-07-21T15:27:40.373522Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    " # Encode labels\n",
    "le = LabelEncoder()\n",
    "fe_train_df['gesture_int'] = le.fit_transform(fe_train_df['gesture'])\n",
    "gesture_classes = le.classes_\n",
    "np.save(output_dir / \"gesture_classes.npy\", gesture_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T15:27:40.489971Z",
     "iopub.status.busy": "2025-07-21T15:27:40.489773Z",
     "iopub.status.idle": "2025-07-21T15:27:40.745911Z",
     "shell.execute_reply": "2025-07-21T15:27:40.745238Z",
     "shell.execute_reply.started": "2025-07-21T15:27:40.489954Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get categorical columns\n",
    "cat_features = fe_train_df.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T15:27:40.746871Z",
     "iopub.status.busy": "2025-07-21T15:27:40.746648Z",
     "iopub.status.idle": "2025-07-21T15:27:46.897556Z",
     "shell.execute_reply": "2025-07-21T15:27:46.896975Z",
     "shell.execute_reply.started": "2025-07-21T15:27:40.746855Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dictionary to store all encoder mappings\n",
    "encoder_mappings = {}\n",
    "\n",
    "print(\"Label Encoding Categorical Features: \",end=\"\")\n",
    "for c in cat_features:\n",
    "    print(f\"{c}, \",end=\"\")\n",
    "    fe_train_df[c] =  fe_train_df[c].astype('category')\n",
    "    \n",
    "    full_le = LabelEncoder()\n",
    "    fe_train_df[c] = full_le.fit_transform(fe_train_df[c].astype(str))\n",
    "\n",
    "    # Save the mapping of numerical values to string labels\n",
    "    encoder_mappings[c] = {\n",
    "        i: label for i, label in enumerate(full_le.classes_)\n",
    "    }\n",
    "\n",
    "with open('new_categorical_encoder_mappings.json', 'w') as f:\n",
    "    json.dump(encoder_mappings, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T15:27:46.898465Z",
     "iopub.status.busy": "2025-07-21T15:27:46.898244Z",
     "iopub.status.idle": "2025-07-21T15:27:46.902814Z",
     "shell.execute_reply": "2025-07-21T15:27:46.902313Z",
     "shell.execute_reply.started": "2025-07-21T15:27:46.898447Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the plot output path\n",
    "plot_output_path = Path('plots')\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "plot_output_path.mkdir(exist_ok=True)\n",
    "print(f\"Directory '{plot_output_path}' is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T15:27:46.903803Z",
     "iopub.status.busy": "2025-07-21T15:27:46.903590Z",
     "iopub.status.idle": "2025-07-21T15:27:46.929645Z",
     "shell.execute_reply": "2025-07-21T15:27:46.928980Z",
     "shell.execute_reply.started": "2025-07-21T15:27:46.903788Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ttest analysis\n",
    "def perform_ttest_analysis(df, group_col, group1_value, group2_value, output_path):  \n",
    "    # Filter data for the two groups\n",
    "    group1_data = df[df[group_col] == group1_value].copy()\n",
    "    group2_data = df[df[group_col] == group2_value].copy()\n",
    "    \n",
    "    print(f\"Group 1 ({group1_value}): {len(group1_data)} records\")\n",
    "    print(f\"Group 2 ({group2_value}): {len(group2_data)} records\")\n",
    "    \n",
    "    # Get numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns.tolist()\n",
    "    \n",
    "    # Remove the group column if it's numerical\n",
    "    if group_col in numerical_cols:\n",
    "        numerical_cols.remove(group_col)\n",
    "    \n",
    "    print(f\"Testing {len(numerical_cols)} numerical variables\")\n",
    "    \n",
    "    # Initialize results list\n",
    "    results = []\n",
    "    \n",
    "    # Perform t-tests for each numerical column\n",
    "    for col in tqdm(numerical_cols, desc=\"Running t-tests and creating plots\"):\n",
    "        try:\n",
    "            # Get data for both groups for this column (remove NaN values)\n",
    "            group1_values = group1_data[col].dropna()\n",
    "            group2_values = group2_data[col].dropna()\n",
    "            \n",
    "            # Skip if insufficient data\n",
    "            if len(group1_values) < 2 or len(group2_values) < 2:\n",
    "                print(f\"Warning: Insufficient data for {col} (Group1: {len(group1_values)}, Group2: {len(group2_values)})\")\n",
    "                continue\n",
    "            \n",
    "            # Perform Levene's test for equal variances\n",
    "            levene_stat, levene_p = stats.levene(group1_values, group2_values)\n",
    "            equal_var = levene_p > 0.05\n",
    "            \n",
    "            # Perform independent t-test\n",
    "            t_stat, p_value = stats.ttest_ind(group1_values, group2_values, equal_var=equal_var)\n",
    "            \n",
    "            # Calculate effect size (Cohen's d)\n",
    "            if equal_var:\n",
    "                pooled_std = np.sqrt(((len(group1_values) - 1) * group1_values.var() + \n",
    "                                     (len(group2_values) - 1) * group2_values.var()) / \n",
    "                                    (len(group1_values) + len(group2_values) - 2))\n",
    "            else:\n",
    "                pooled_std = np.sqrt((group1_values.var() + group2_values.var()) / 2)\n",
    "            \n",
    "            cohens_d = (group1_values.mean() - group2_values.mean()) / pooled_std if pooled_std != 0 else 0\n",
    "            \n",
    "            # Calculate 95% confidence interval for the difference in means\n",
    "            diff_mean = group2_values.mean() - group1_values.mean()\n",
    "            se_diff = np.sqrt(group1_values.var()/len(group1_values) + group2_values.var()/len(group2_values))\n",
    "            t_critical = stats.t.ppf(0.975, len(group1_values) + len(group2_values) - 2)\n",
    "            ci_lower = diff_mean - t_critical * se_diff\n",
    "            ci_upper = diff_mean + t_critical * se_diff\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'variable': col,\n",
    "                'group1_mean': group1_values.mean(),\n",
    "                'group1_std': group1_values.std(),\n",
    "                'group1_median': group1_values.median(),\n",
    "                'group1_count': len(group1_values),\n",
    "                'group2_mean': group2_values.mean(),\n",
    "                'group2_std': group2_values.std(),\n",
    "                'group2_median': group2_values.median(),\n",
    "                'group2_count': len(group2_values),\n",
    "                'mean_difference': diff_mean,\n",
    "                'ci_lower': ci_lower,\n",
    "                'ci_upper': ci_upper,\n",
    "                't_statistic': t_stat,\n",
    "                'p_value': p_value,\n",
    "                'cohens_d': cohens_d,\n",
    "                'equal_variances': equal_var,\n",
    "                'levene_p_value': levene_p,\n",
    "                'significant_005': p_value < 0.05,\n",
    "                'significant_001': p_value < 0.01,\n",
    "                'significant_0001': p_value < 0.001\n",
    "            })\n",
    "            \n",
    "            # Create comparison plot\n",
    "            create_comparison_plot(group_col, group1_values, group2_values, col, \n",
    "                                 group1_value, group2_value, \n",
    "                                 t_stat, p_value, cohens_d, output_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {col}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    if len(results_df) > 0:\n",
    "        # Sort by p-value\n",
    "        results_df = results_df.sort_values('p_value').reset_index(drop=True)\n",
    "        \n",
    "        # Add interpretation columns\n",
    "        results_df['effect_size_interpretation'] = results_df['cohens_d'].abs().apply(interpret_cohens_d)\n",
    "        results_df['significance_level'] = results_df.apply(get_significance_level, axis=1)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def interpret_cohens_d(d):\n",
    "    \"\"\"Interpret Cohen's d effect size\"\"\"\n",
    "    d_abs = abs(d)\n",
    "    if d_abs < 0.2:\n",
    "        return \"negligible\"\n",
    "    elif d_abs < 0.5:\n",
    "        return \"small\"\n",
    "    elif d_abs < 0.8:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"large\"\n",
    "\n",
    "def get_significance_level(row):\n",
    "    \"\"\"Get significance level string\"\"\"\n",
    "    if row['p_value'] < 0.001:\n",
    "        return \"*** (p < 0.001)\"\n",
    "    elif row['p_value'] < 0.01:\n",
    "        return \"** (p < 0.01)\"\n",
    "    elif row['p_value'] < 0.05:\n",
    "        return \"* (p < 0.05)\"\n",
    "    else:\n",
    "        return \"Not significant\"\n",
    "\n",
    "def create_comparison_plot(group_col, group1_values, group2_values, col_name, \n",
    "                          group1_name, group2_name, t_stat, p_value, cohens_d, output_path):\n",
    "    \"\"\"Create comprehensive comparison plot for two groups\"\"\"\n",
    "    \n",
    "    # Set up the figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f'Statistical Comparison: {col_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Color scheme\n",
    "    color1 = '#ff7f7f'  # Light red for group 1\n",
    "    color2 = '#7fbfff'  # Light blue for group 2\n",
    "    \n",
    "    # 1. Histogram comparison (top left)\n",
    "    ax1 = axes[0, 0]\n",
    "    \n",
    "    # Calculate bins for better visualization\n",
    "    all_values = np.concatenate([group1_values, group2_values])\n",
    "    bins = np.histogram_bin_edges(all_values, bins=30)\n",
    "    \n",
    "    ax1.hist(group1_values, bins=bins, alpha=0.7, label=f'{group1_name} (n={len(group1_values)})', \n",
    "             color=color1, density=True, edgecolor='black', linewidth=0.5)\n",
    "    ax1.hist(group2_values, bins=bins, alpha=0.7, label=f'{group2_name} (n={len(group2_values)})', \n",
    "             color=color2, density=True, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # Add mean lines\n",
    "    ax1.axvline(group1_values.mean(), color='darkred', linestyle='--', linewidth=2, \n",
    "                label=f'{group1_name} Mean: {group1_values.mean():.2f}')\n",
    "    ax1.axvline(group2_values.mean(), color='darkblue', linestyle='--', linewidth=2,\n",
    "                label=f'{group2_name} Mean: {group2_values.mean():.2f}')\n",
    "    \n",
    "    ax1.set_title('Distribution Comparison (Histogram)')\n",
    "    ax1.set_xlabel(col_name)\n",
    "    ax1.set_ylabel('Density')\n",
    "    ax1.legend(fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Box plot comparison (top right)\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    box_data = [group1_values, group2_values]\n",
    "    box_labels = [f'{group1_name}\\n(n={len(group1_values)})', f'{group2_name}\\n(n={len(group2_values)})']\n",
    "    \n",
    "    bp = ax2.boxplot(box_data, labels=box_labels, patch_artist=True, \n",
    "                     boxprops=dict(alpha=0.7), showfliers=True)\n",
    "    bp['boxes'][0].set_facecolor(color1)\n",
    "    bp['boxes'][1].set_facecolor(color2)\n",
    "    \n",
    "    ax2.set_title('Box Plot Comparison')\n",
    "    ax2.set_ylabel(col_name)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. KDE plot (bottom left)\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    try:\n",
    "        # Only plot KDE if we have enough data points\n",
    "        if len(group1_values) > 3:\n",
    "            sns.kdeplot(data=group1_values, label=f'{group1_name}', \n",
    "                       color='darkred', ax=ax3, linewidth=2)\n",
    "        if len(group2_values) > 3:\n",
    "            sns.kdeplot(data=group2_values, label=f'{group2_name}', \n",
    "                       color='darkblue', ax=ax3, linewidth=2)\n",
    "    except:\n",
    "        # Fallback to simple line plot if KDE fails\n",
    "        ax3.hist(group1_values, alpha=0.5, density=True, color=color1, label=group1_name)\n",
    "        ax3.hist(group2_values, alpha=0.5, density=True, color=color2, label=group2_name)\n",
    "    \n",
    "    ax3.set_title('Density Comparison (KDE)')\n",
    "    ax3.set_xlabel(col_name)\n",
    "    ax3.set_ylabel('Density')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Statistics summary (bottom right)\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Calculate additional statistics\n",
    "    effect_size_interp = interpret_cohens_d(cohens_d)\n",
    "    sig_level = get_significance_level({'p_value': p_value})\n",
    "    \n",
    "    stats_text = f'T-Test Results:\\n'\n",
    "    stats_text += f'\\n'\n",
    "    stats_text += f'T-statistic: {t_stat:.4f}\\n'\n",
    "    stats_text += f'P-value: {p_value:.2e}\\n'\n",
    "    stats_text += f'Cohen\\'s d: {cohens_d:.4f} ({effect_size_interp})\\n'\n",
    "    stats_text += f'Significance: {sig_level}\\n\\n'\n",
    "    \n",
    "    stats_text += f'{group1_name}:\\n'\n",
    "    stats_text += f'  Mean  SD: {group1_values.mean():.2f}  {group1_values.std():.2f}\\n'\n",
    "    stats_text += f'  Median: {group1_values.median():.2f}\\n'\n",
    "    stats_text += f'  Range: [{group1_values.min():.2f}, {group1_values.max():.2f}]\\n'\n",
    "    stats_text += f'  Count: {len(group1_values)}\\n\\n'\n",
    "    \n",
    "    stats_text += f'{group2_name}:\\n'\n",
    "    stats_text += f'  Mean  SD: {group2_values.mean():.2f}  {group2_values.std():.2f}\\n'\n",
    "    stats_text += f'  Median: {group2_values.median():.2f}\\n'\n",
    "    stats_text += f'  Range: [{group2_values.min():.2f}, {group2_values.max():.2f}]\\n'\n",
    "    stats_text += f'  Count: {len(group2_values)}\\n\\n'\n",
    "    \n",
    "    # Difference\n",
    "    mean_diff = group2_values.mean() - group1_values.mean()\n",
    "    stats_text += f'Mean Difference: {mean_diff:.2f}\\n'\n",
    "    stats_text += f'({group2_name} - {group1_name})\\n\\n'\n",
    "    \n",
    "    # Effect size interpretation\n",
    "    stats_text += f'Effect Size Interpretation:\\n'\n",
    "    if abs(cohens_d) < 0.2:\n",
    "        stats_text += f'Negligible practical difference'\n",
    "    elif abs(cohens_d) < 0.5:\n",
    "        stats_text += f'Small practical difference'\n",
    "    elif abs(cohens_d) < 0.8:\n",
    "        stats_text += f'Medium practical difference'\n",
    "    else:\n",
    "        stats_text += f'Large practical difference'\n",
    "    \n",
    "    ax4.text(0.05, 0.95, stats_text,\n",
    "             transform=ax4.transAxes,\n",
    "             verticalalignment='top',\n",
    "             horizontalalignment='left',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8),\n",
    "             fontsize=10, fontfamily='monospace')\n",
    "    ax4.set_xlim(0, 1)\n",
    "    ax4.set_ylim(0, 1)\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save with significance indicator in filename\n",
    "    if p_value < 0.001:\n",
    "        sig_suffix = \"_highly_significant\"\n",
    "    elif p_value < 0.05:\n",
    "        sig_suffix = \"_significant\"\n",
    "    else:\n",
    "        sig_suffix = \"_not_significant\"\n",
    "    \n",
    "    plt.savefig(f'{output_path}/{group_col}_{col_name}_ttest_{group1_name}_vs_{group2_name}_{sig_suffix}_{effect_size_interp}.png', \n",
    "                dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Perform the analysis\n",
    "#ttest_results = perform_ttest_analysis(\n",
    "#    df=fe_train_df, \n",
    "#    group_col='performs_gesture',\n",
    "#    group1_value=True,\n",
    "#    group2_value=False,\n",
    "#    output_path=plot_output_path\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-21T17:27:31.799Z",
     "iopub.execute_input": "2025-07-21T15:27:46.931011Z",
     "iopub.status.busy": "2025-07-21T15:27:46.930407Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if train:\n",
    "    print(\"TRIANING MODE\")\n",
    "    \n",
    "    # Define feature columns\n",
    "    imu_cols = ['acc_x', \n",
    "                'acc_y', \n",
    "                'acc_z', \n",
    "                'linear_acc_x', \n",
    "                'linear_acc_y',\n",
    "                'linear_acc_z',\n",
    "                'rot_x', \n",
    "                'rot_y', \n",
    "                'rot_z', \n",
    "                'rot_w',\n",
    "                'acc_mag', \n",
    "                'linear_acc_mag', \n",
    "                'linear_acc_mag_jerk',\n",
    "                'angular_vel_x', \n",
    "                'angular_vel_y', \n",
    "                'angular_vel_z',\n",
    "               # added columns\n",
    "                'acc_y_rolling_mean',\n",
    "                'acc_y_rolling_std',\n",
    "                'acc_z_rolling_mean',\n",
    "                'acc_z_rolling_std',\n",
    "                'rot_w_rolling_mean',\n",
    "                'rot_w_rolling_std',\n",
    "                'rot_z_rolling_mean',\n",
    "                'rot_z_w_product',\n",
    "                'euler_x',\n",
    "                'euler_y',\n",
    "                'euler_z',\n",
    "               ]\n",
    "    \n",
    "    thm_cols = [\n",
    "        'thm_1',\n",
    "        'thm_2_delta_from_mean',\n",
    "        'thm_2_normalized',\n",
    "        'thm_2_rolling_std',\n",
    "        'thm_2',\n",
    "        'thm_3',\n",
    "        'thm_4',\n",
    "        'thm_5',\n",
    "    ]\n",
    "    tof_aggregated_cols = [\n",
    "        'tof_5_band5_mean',\n",
    "        'tof_5_band4_mean',\n",
    "        'tof_5_band1_mean',\n",
    "        'tof_4_band9_mean',\n",
    "        'tof_4_band8_mean',\n",
    "        'tof_4_band7_mean',\n",
    "        'tof_4_band6_mean',\n",
    "        'tof_4_band5_mean',\n",
    "        'tof_4_band4_mean',\n",
    "        'tof_4_band1_mean',\n",
    "        'tof_3_band2_mean',\n",
    "        'tof_3_band1_mean',\n",
    "        'tof_2_band8_mean',\n",
    "        'tof_2_band7_mean',\n",
    "        'tof_2_band6_mean',\n",
    "        'tof_2_band5_mean',\n",
    "        'tof_2_band4_mean',\n",
    "        'tof_2_band3_mean',\n",
    "        'tof_2_band2_mean',\n",
    "        'tof_2_band1_mean',\n",
    "        'tof_1_band8_mean',\n",
    "        'tof_1_band7_mean',\n",
    "        'tof_1_band6_mean',\n",
    "        'tof_1_band5_mean',\n",
    "        'tof_1_band4_mean',\n",
    "        'tof_1_band3_mean',\n",
    "        'tof_1_band2_mean',\n",
    "    ]\n",
    "    for i in range(1, 6):\n",
    "        tof_aggregated_cols.extend([\n",
    "            f'tof_{i}_mean', f'tof_{i}_std', f'tof_{i}_min', f'tof_{i}_max'\n",
    "        ])\n",
    "    \n",
    "    final_feature_cols = imu_cols + thm_cols + tof_aggregated_cols\n",
    "    imu_dim = len(imu_cols)\n",
    "    tof_thm_dim = len(thm_cols) + len(tof_aggregated_cols)\n",
    "    \n",
    "    print(f\"IMU features: {imu_dim} | THM + ToF features: {tof_thm_dim}\")\n",
    "    np.save(output_dir / \"feature_cols.npy\", np.array(final_feature_cols))\n",
    "    \n",
    "    # Build sequences efficiently\n",
    "    print(\"Building sequences...\")\n",
    "    seq_gp = fe_train_df.groupby('sequence_id')\n",
    "    X_list_unscaled = []\n",
    "    y_list_int = []\n",
    "    groups_list = []\n",
    "    lens = []\n",
    "    \n",
    "    for seq_id, seq_df in seq_gp:\n",
    "        X_list_unscaled.append(\n",
    "            seq_df[final_feature_cols].fillna(0).values.astype('float32')\n",
    "        )\n",
    "        y_list_int.append(seq_df['gesture_int'].iloc[0])\n",
    "        groups_list.append(seq_df['subject'].iloc[0])\n",
    "        lens.append(len(seq_df))\n",
    "    \n",
    "    # Scaling\n",
    "    print(\"Fitting StandardScaler...\")\n",
    "    all_steps_concatenated = np.concatenate(X_list_unscaled, axis=0)\n",
    "    scaler = StandardScaler().fit(all_steps_concatenated)\n",
    "    joblib.dump(scaler, output_dir / \"scaler.pkl\")\n",
    "    \n",
    "    # Scale and pad\n",
    "    X_scaled_list = [scaler.transform(x_seq) for x_seq in X_list_unscaled]\n",
    "    pad_len = int(np.percentile(lens, pad_percentile))\n",
    "    np.save(output_dir / \"sequence_maxlen.npy\", pad_len)\n",
    "    \n",
    "    X = pad_sequences(X_scaled_list, maxlen=pad_len, padding='post', \n",
    "                      truncating='post', dtype='float32')\n",
    "    y_stratify = np.array(y_list_int)\n",
    "    groups = np.array(groups_list)\n",
    "    y = to_categorical(y_list_int, num_classes=len(le.classes_))\n",
    "    \n",
    "    print(f\"Final data shape: X={X.shape}, y={y.shape}\")\n",
    "    \n",
    "    # Cross-validation with reduced folds\n",
    "    print(f\"\\nStarting training with {n_splits}-fold CV...\")\n",
    "    sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(sgkf.split(X, y_stratify, groups)):\n",
    "        print(f\"\\n{'='*20} FOLD {fold+1}/{n_splits} {'='*20}\")\n",
    "        \n",
    "        X_tr, X_val = X[train_idx], X[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Build model\n",
    "        model = build_two_branch_model_optimized(\n",
    "            pad_len=pad_len, \n",
    "            imu_dim=imu_dim, \n",
    "            tof_dim=tof_thm_dim, \n",
    "            n_classes=len(le.classes_), \n",
    "            wd=wd\n",
    "        )\n",
    "        \n",
    "        # Compile with mixed precision optimizer\n",
    "        opt = Adam(learning_rate=lr_init)\n",
    "        opt = mixed_precision.LossScaleOptimizer(opt)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=opt,\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Simplified callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss', \n",
    "                patience=patience, \n",
    "                restore_best_weights=True, \n",
    "                verbose=1\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                factor=0.5, \n",
    "                patience=10, \n",
    "                min_lr=1e-6, \n",
    "                verbose=1\n",
    "            ),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                str(output_dir / f'model_fold_{fold}_best.h5'),\n",
    "                save_best_only=True,\n",
    "                monitor='val_loss'\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train with larger batch size\n",
    "        train_gen = FastMixupGenerator(\n",
    "            X_tr, y_tr, \n",
    "            batch_size=batch_size, \n",
    "            alpha=mixup_alpha\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_gen,\n",
    "            epochs=epochs,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Save model\n",
    "        model.save(output_dir / f\"model_fold_{fold}_final.h5\")\n",
    "        \n",
    "        # Clear session\n",
    "        tf.keras.backend.clear_session()\n",
    "    \n",
    "    print(\"\\nTraining completed!\")\n",
    "else:\n",
    "    # INFERENCE MODE\n",
    "    print(\"INFERENCE MODE  Loading artifacts from\", pretrained_dir)\n",
    "    \n",
    "    # Load pretrained artifacts\n",
    "    final_feature_cols = np.load(pretrained_dir / \"feature_cols.npy\", allow_pickle=True).tolist()\n",
    "    pad_len = int(np.load(pretrained_dir / \"sequence_maxlen.npy\"))\n",
    "    scaler = joblib.load(pretrained_dir / \"scaler.pkl\")\n",
    "    gesture_classes = np.load(pretrained_dir / \"gesture_classes.npy\", allow_pickle=True)\n",
    "    \n",
    "    # Extract dimensions from feature columns\n",
    "    imu_cols = ['acc_x', 'acc_y', 'acc_z', \n",
    "                'linear_acc_x', 'linear_acc_y', 'linear_acc_z',\n",
    "                'rot_x', 'rot_y', 'rot_z', 'rot_w',\n",
    "                'acc_mag', 'linear_acc_mag', 'linear_acc_mag_jerk',\n",
    "                'angular_vel_x', 'angular_vel_y', 'angular_vel_z']\n",
    "    imu_dim = len(imu_cols)\n",
    "    tof_thm_dim = len(final_feature_cols) - imu_dim\n",
    "    \n",
    "    print(f\"Loaded artifacts:\")\n",
    "    print(f\"  - Feature columns: {len(final_feature_cols)}\")\n",
    "    print(f\"  - Sequence max length: {pad_len}\")\n",
    "    print(f\"  - Gesture classes: {len(gesture_classes)}\")\n",
    "    print(f\"  - IMU dim: {imu_dim}, ToF+THM dim: {tof_thm_dim}\")\n",
    "    \n",
    "    # Disable mixed precision for inference to avoid Cast layer issues\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    mixed_precision.set_global_policy('float32')\n",
    "    \n",
    "    # Define Cast layer for loading models trained with mixed precision\n",
    "    class Cast(tf.keras.layers.Layer):\n",
    "        def __init__(self, dtype='float32', **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.target_dtype = dtype\n",
    "            \n",
    "        def call(self, inputs):\n",
    "            return tf.cast(inputs, self.target_dtype)\n",
    "        \n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({'dtype': self.target_dtype})\n",
    "            return config\n",
    "    \n",
    "    # Custom objects for model loading\n",
    "    custom_objs = {\n",
    "        'time_sum': time_sum,\n",
    "        'squeeze_last_axis': squeeze_last_axis,\n",
    "        'expand_last_axis': expand_last_axis,\n",
    "        'se_block': se_block,\n",
    "        'residual_se_cnn_block': residual_se_cnn_block,\n",
    "        'attention_layer': attention_layer,\n",
    "        'Cast': Cast  # Add proper Cast layer support\n",
    "    }\n",
    "    \n",
    "    # Load models\n",
    "    models = []\n",
    "    print(f\"\\n Loading {n_splits} models for ensemble inference...\")\n",
    "    for fold in range(n_splits):\n",
    "        model_path = pretrained_dir / f\"model_fold_{fold}_final.h5\"\n",
    "        print(f\"  Loading model: {model_path}\")\n",
    "        model = load_model(model_path, compile=False, custom_objects=custom_objs)\n",
    "        models.append(model)\n",
    "    print(f\"[INFO] Successfully loaded {len(models)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-21T17:27:31.806Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Kaggle evaluation server setup\n",
    "# import kaggle_evaluation.cmi_inference_server\n",
    "# inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n",
    "\n",
    "# if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "#    inference_server.serve()\n",
    "#else:\n",
    "#    print(\"\\n Running local gateway for testing...\")\n",
    "#    inference_server.run_local_gateway(\n",
    "#        data_paths=(\n",
    "#            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv',\n",
    "#            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv',\n",
    "#        )\n",
    "#    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12518947,
     "sourceId": 102335,
     "sourceType": "competition"
    },
    {
     "sourceId": 242561727,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 403270,
     "modelInstanceId": 383852,
     "sourceId": 477290,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
