{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-11T15:18:11.785890Z",
     "iopub.status.busy": "2025-07-11T15:18:11.785298Z",
     "iopub.status.idle": "2025-07-11T15:18:25.841345Z",
     "shell.execute_reply": "2025-07-11T15:18:25.840555Z",
     "shell.execute_reply.started": "2025-07-11T15:18:11.785859Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, json, joblib, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings \n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence, to_categorical, pad_sequences\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, BatchNormalization, Activation, add, MaxPooling1D, Dropout,\n",
    "    Bidirectional, LSTM, GlobalAveragePooling1D, Dense, Multiply, Reshape,\n",
    "    Lambda, Concatenate, GRU, GaussianNoise, Add, GlobalMaxPooling1D,\n",
    "    MultiHeadAttention, LayerNormalization\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import polars as pl\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from cmi_2025_metric_copy_for_import import CompetitionMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T15:18:25.842891Z",
     "iopub.status.busy": "2025-07-11T15:18:25.842429Z",
     "iopub.status.idle": "2025-07-11T15:18:25.847306Z",
     "shell.execute_reply": "2025-07-11T15:18:25.846547Z",
     "shell.execute_reply.started": "2025-07-11T15:18:25.842872Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.experimental.numpy.random.seed(seed)\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:34:55.557690Z",
     "iopub.status.busy": "2025-07-11T16:34:55.557127Z",
     "iopub.status.idle": "2025-07-11T16:34:55.562441Z",
     "shell.execute_reply": "2025-07-11T16:34:55.561597Z",
     "shell.execute_reply.started": "2025-07-11T16:34:55.557669Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports ready\n"
     ]
    }
   ],
   "source": [
    "train = True                    \n",
    "raw_dir = Path(\"input\")\n",
    "pretrained_dir = Path(\"input/cmi-d-111\")\n",
    "output_dir = Path(\"./\")                                    \n",
    "batch_size = 64\n",
    "pad_percentile = 95\n",
    "lr_init = 5e-4\n",
    "wd = 3e-3\n",
    "mixup_alpha = 0.4\n",
    "epochs = 220\n",
    "patience = 40\n",
    "\n",
    "print(\"Imports ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T15:18:25.879075Z",
     "iopub.status.busy": "2025-07-11T15:18:25.878765Z",
     "iopub.status.idle": "2025-07-11T15:18:25.891019Z",
     "shell.execute_reply": "2025-07-11T15:18:25.890484Z",
     "shell.execute_reply.started": "2025-07-11T15:18:25.879048Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Tensor Manipulations\n",
    "def time_sum(x):\n",
    "    return K.sum(x, axis=1)\n",
    "\n",
    "def squeeze_last_axis(x):\n",
    "    return tf.squeeze(x, axis=-1)\n",
    "\n",
    "def expand_last_axis(x):\n",
    "    return tf.expand_dims(x, axis=-1)\n",
    "\n",
    "def se_block(x, reduction=8):\n",
    "    ch = x.shape[-1]\n",
    "    se = GlobalAveragePooling1D()(x)\n",
    "    se = Dense(ch // reduction, activation='relu')(se)\n",
    "    se = Dense(ch, activation='sigmoid')(se)\n",
    "    se = Reshape((1, ch))(se)\n",
    "    return Multiply()([x, se])\n",
    "\n",
    "# Residual CNN Block with SE\n",
    "def residual_se_cnn_block(x, filters, kernel_size, pool_size=2, drop=0.3, wd=1e-4):\n",
    "    shortcut = x\n",
    "    for _ in range(2):\n",
    "        x = Conv1D(filters, kernel_size, padding='same', use_bias=False,\n",
    "                   kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "    x = se_block(x)\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, 1, padding='same', use_bias=False,\n",
    "                          kernel_regularizer=l2(wd))(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    x = add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(pool_size)(x)\n",
    "    x = Dropout(drop)(x)\n",
    "    return x\n",
    "\n",
    "def attention_layer(inputs):\n",
    "    score = Dense(1, activation='tanh')(inputs)\n",
    "    score = Lambda(squeeze_last_axis)(score)\n",
    "    weights = Activation('softmax')(score)\n",
    "    weights = Lambda(expand_last_axis)(weights)\n",
    "    context = Multiply()([inputs, weights])\n",
    "    context = Lambda(time_sum)(context)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:00:44.284385Z",
     "iopub.status.busy": "2025-07-11T16:00:44.284105Z",
     "iopub.status.idle": "2025-07-11T16:00:44.294488Z",
     "shell.execute_reply": "2025-07-11T16:00:44.293686Z",
     "shell.execute_reply.started": "2025-07-11T16:00:44.284365Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EnhancedMixupGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size, alpha=0.4, augment=True):\n",
    "        self.X, self.y = X, y\n",
    "        self.batch = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.augment = augment\n",
    "        self.indices = np.arange(len(X))\n",
    "        self.n_imu_features = 24  # Based on your IMU feature count\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch))\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        idx = self.indices[i*self.batch:(i+1)*self.batch]\n",
    "        Xb, yb = self.X[idx].copy(), self.y[idx].copy()\n",
    "        \n",
    "        if self.augment:\n",
    "            # Time shift instead of warping (simpler)\n",
    "            if np.random.rand() > 0.5:\n",
    "                shift = np.random.randint(-5, 6)  # Shift by up to 5 timesteps\n",
    "                if shift > 0:\n",
    "                    Xb[:, shift:, :] = Xb[:, :-shift, :]\n",
    "                    Xb[:, :shift, :] = 0\n",
    "                elif shift < 0:\n",
    "                    Xb[:, :shift, :] = Xb[:, -shift:, :]\n",
    "                    Xb[:, shift:, :] = 0\n",
    "                    \n",
    "            # Magnitude scaling - only for IMU features\n",
    "            if np.random.rand() > 0.5:\n",
    "                scale = np.random.uniform(0.8, 1.2)\n",
    "                Xb[:, :, :self.n_imu_features] *= scale\n",
    "                \n",
    "            # Add noise - only to IMU features\n",
    "            if np.random.rand() > 0.5:\n",
    "                noise = np.random.normal(0, 0.05, \n",
    "                                       (len(Xb), Xb.shape[1], self.n_imu_features))\n",
    "                Xb[:, :, :self.n_imu_features] += noise\n",
    "                \n",
    "            # Random feature dropout for ToF/thermal features\n",
    "            if np.random.rand() > 0.5:\n",
    "                # Randomly drop some ToF/thermal features\n",
    "                n_drop = np.random.randint(1, 6)\n",
    "                drop_indices = np.random.choice(\n",
    "                    range(self.n_imu_features, Xb.shape[2]), \n",
    "                    size=n_drop, \n",
    "                    replace=False\n",
    "                )\n",
    "                Xb[:, :, drop_indices] = 0\n",
    "        \n",
    "        # Mixup\n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        perm = np.random.permutation(len(Xb))\n",
    "        X_mix = lam * Xb + (1-lam) * Xb[perm]\n",
    "        y_mix = lam * yb + (1-lam) * yb[perm]\n",
    "        \n",
    "        return X_mix.astype('float32'), y_mix.astype('float32')\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T15:19:21.913279Z",
     "iopub.status.busy": "2025-07-11T15:19:21.912563Z",
     "iopub.status.idle": "2025-07-11T15:19:21.918546Z",
     "shell.execute_reply": "2025-07-11T15:19:21.917907Z",
     "shell.execute_reply.started": "2025-07-11T15:19:21.913253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cosine annealing with warm restarts\n",
    "def cosine_annealing_with_warmup(epoch, lr, warmup_epochs=10, total_epochs=160, min_lr=1e-6):\n",
    "    if epoch < warmup_epochs:\n",
    "        return lr_init * (epoch + 1) / warmup_epochs\n",
    "    else:\n",
    "        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n",
    "        return min_lr + (lr_init - min_lr) * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "\n",
    "# Create learning rate scheduler\n",
    "lr_scheduler = LearningRateScheduler(lambda epoch: cosine_annealing_with_warmup(epoch, lr_init))\n",
    "\n",
    "# Training callbacks\n",
    "training_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True),\n",
    "    lr_scheduler,\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-7, verbose=1),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        str(output_dir / 'best_model_{epoch:02d}_{val_loss:.4f}.h5'),\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss'\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T15:19:35.406101Z",
     "iopub.status.busy": "2025-07-11T15:19:35.405397Z",
     "iopub.status.idle": "2025-07-11T15:19:35.414455Z",
     "shell.execute_reply": "2025-07-11T15:19:35.413802Z",
     "shell.execute_reply.started": "2025-07-11T15:19:35.406074Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def remove_gravity_from_acc(acc_data, rot_data):\n",
    "\n",
    "    if isinstance(acc_data, pd.DataFrame):\n",
    "        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    else:\n",
    "        acc_values = acc_data\n",
    "\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = acc_values.shape[0]\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    \n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
    "            linear_accel[i, :] = acc_values[i, :] \n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except ValueError:\n",
    "             linear_accel[i, :] = acc_values[i, :]\n",
    "             \n",
    "    return linear_accel\n",
    "\n",
    "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200): # Assuming 200Hz sampling rate\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_vel = np.zeros((num_samples, 3))\n",
    "\n",
    "    for i in range(num_samples - 1):\n",
    "        q_t = quat_values[i]\n",
    "        q_t_plus_dt = quat_values[i+1]\n",
    "\n",
    "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
    "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "\n",
    "            # Calculate the relative rotation\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            \n",
    "            # Convert delta rotation to angular velocity vector\n",
    "            # The rotation vector (Euler axis * angle) scaled by 1/dt\n",
    "            # is a good approximation for small delta_rot\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except ValueError:\n",
    "            # If quaternion is invalid, angular velocity remains zero\n",
    "            pass\n",
    "            \n",
    "    return angular_vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T15:19:40.836355Z",
     "iopub.status.busy": "2025-07-11T15:19:40.836081Z",
     "iopub.status.idle": "2025-07-11T15:19:40.842737Z",
     "shell.execute_reply": "2025-07-11T15:19:40.841779Z",
     "shell.execute_reply.started": "2025-07-11T15:19:40.836336Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_angular_distance(rot_data):\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_dist = np.zeros(num_samples)\n",
    "\n",
    "    for i in range(num_samples - 1):\n",
    "        q1 = quat_values[i]\n",
    "        q2 = quat_values[i+1]\n",
    "\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n",
    "           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
    "            angular_dist[i] = 0 \n",
    "            continue\n",
    "        try:\n",
    "            \n",
    "            r1 = R.from_quat(q1)\n",
    "            r2 = R.from_quat(q2)\n",
    "\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            \n",
    " \n",
    "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "            angular_dist[i] = angle\n",
    "        except ValueError:\n",
    "            angular_dist[i] = 0 \n",
    "            pass\n",
    "            \n",
    "    return angular_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T15:19:43.299407Z",
     "iopub.status.busy": "2025-07-11T15:19:43.298749Z",
     "iopub.status.idle": "2025-07-11T15:19:43.303851Z",
     "shell.execute_reply": "2025-07-11T15:19:43.302925Z",
     "shell.execute_reply.started": "2025-07-11T15:19:43.299382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def SEBlock(ratio=16):\n",
    "    def block(inputs):\n",
    "        channels = inputs.shape[-1]\n",
    "        x = GlobalAveragePooling1D()(inputs)\n",
    "        x = Dense(channels // ratio, activation='relu')(x)\n",
    "        x = Dense(channels, activation='sigmoid')(x)\n",
    "        x = Reshape((1, channels))(x)\n",
    "        return Multiply()([inputs, x])\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T15:19:48.372040Z",
     "iopub.status.busy": "2025-07-11T15:19:48.371372Z",
     "iopub.status.idle": "2025-07-11T15:19:48.377181Z",
     "shell.execute_reply": "2025-07-11T15:19:48.376533Z",
     "shell.execute_reply.started": "2025-07-11T15:19:48.371999Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_enhanced_imu_features(acc_data, rot_data, sampling_rate=200):\n",
    "    # Get gravity-compensated acceleration\n",
    "    linear_acc = remove_gravity_from_acc(acc_data, rot_data)\n",
    "    \n",
    "    # Angular velocity\n",
    "    angular_vel = calculate_angular_velocity_from_quat(rot_data, 1/sampling_rate)\n",
    "    \n",
    "    # Angular distance/displacement\n",
    "    angular_dist = calculate_angular_distance(rot_data)\n",
    "    \n",
    "    # Magnitude features\n",
    "    acc_magnitude = np.linalg.norm(acc_data[['acc_x', 'acc_y', 'acc_z']].values, axis=1)\n",
    "    linear_acc_magnitude = np.linalg.norm(linear_acc, axis=1)\n",
    "    angular_vel_magnitude = np.linalg.norm(angular_vel, axis=1)\n",
    "    \n",
    "    # Jerk (derivative of acceleration)\n",
    "    jerk = np.gradient(linear_acc, axis=0) * sampling_rate\n",
    "    \n",
    "    # Frequency domain features (per window)\n",
    "    # Could add FFT magnitudes for dominant frequencies\n",
    "    \n",
    "    # Combine all features\n",
    "    enhanced_features = np.column_stack([\n",
    "        acc_data[['acc_x', 'acc_y', 'acc_z']].values,  # 3 features\n",
    "        rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values,  # 4 features\n",
    "        linear_acc,  # 3 features\n",
    "        angular_vel,  # 3 features\n",
    "        jerk,  # 3 features\n",
    "        acc_magnitude.reshape(-1, 1),  # 1 feature\n",
    "        linear_acc_magnitude.reshape(-1, 1),  # 1 feature\n",
    "        angular_vel_magnitude.reshape(-1, 1),  # 1 feature\n",
    "        angular_dist.reshape(-1, 1)  # 1 feature\n",
    "    ])  # Total: 20 features instead of 7\n",
    "    \n",
    "    return enhanced_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T15:19:52.517208Z",
     "iopub.status.busy": "2025-07-11T15:19:52.516397Z",
     "iopub.status.idle": "2025-07-11T15:19:55.578320Z",
     "shell.execute_reply": "2025-07-11T15:19:55.577642Z",
     "shell.execute_reply.started": "2025-07-11T15:19:52.517173Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_two_branch_model_v2(pad_len, imu_dim, tof_dim, n_classes, wd=1e-4):\n",
    "    inp = Input(shape=(pad_len, imu_dim+tof_dim))\n",
    "    imu = Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "    tof = Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "    \n",
    "    # Enhanced IMU branch with multi-scale\n",
    "    x1_3 = residual_se_cnn_block(imu, 64, 3, drop=0.1, wd=wd)\n",
    "    x1_5 = residual_se_cnn_block(imu, 64, 5, drop=0.1, wd=wd)\n",
    "    x1_7 = residual_se_cnn_block(imu, 64, 7, drop=0.1, wd=wd)\n",
    "    x1 = Concatenate()([x1_3, x1_5, x1_7])\n",
    "    x1 = Conv1D(128, 1, activation='relu')(x1)  # Channel fusion\n",
    "    x1 = residual_se_cnn_block(x1, 128, 5, drop=0.1, wd=wd)\n",
    "    \n",
    "    # Enhanced TOF branch with 1x1 conv first\n",
    "    x2 = Conv1D(128, 1, padding='same', use_bias=False, kernel_regularizer=l2(wd))(tof)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Activation('relu')(x2)\n",
    "    \n",
    "    # Multi-scale for ToF\n",
    "    x2_3 = Conv1D(96, 3, padding='same', activation='relu')(x2)\n",
    "    x2_5 = Conv1D(96, 5, padding='same', activation='relu')(x2)\n",
    "    x2_7 = Conv1D(96, 7, padding='same', activation='relu')(x2)\n",
    "    x2 = Concatenate()([x2_3, x2_5, x2_7])\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = SEBlock(ratio=16)(x2)\n",
    "    x2 = MaxPooling1D(2)(x2)\n",
    "    x2 = Dropout(0.2)(x2)\n",
    "    \n",
    "    x2 = residual_se_cnn_block(x2, 256, 3, drop=0.2, wd=wd)\n",
    "    \n",
    "    # Cross-modal attention before merging\n",
    "    x1_att = MultiHeadAttention(num_heads=4, key_dim=32)(x1, x2)\n",
    "    x2_att = MultiHeadAttention(num_heads=4, key_dim=32)(x2, x1)\n",
    "    \n",
    "    merged = Concatenate()([x1, x2, x1_att, x2_att])\n",
    "    \n",
    "    # Enhanced temporal modeling\n",
    "    xa = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(wd),\n",
    "                           dropout=0.2, recurrent_dropout=0.2))(merged)\n",
    "    xb = Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(wd),\n",
    "                          dropout=0.2, recurrent_dropout=0.2))(merged)\n",
    "    xc = GaussianNoise(0.09)(merged)\n",
    "    xc = Dense(128, activation='elu')(xc)\n",
    "    \n",
    "    x = Concatenate()([xa, xb, xc])\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # Multiple attention mechanisms\n",
    "    x_att1 = attention_layer(x)\n",
    "    x_att2 = GlobalAveragePooling1D()(x)\n",
    "    x_att3 = GlobalMaxPooling1D()(x)\n",
    "    x = Concatenate()([x_att1, x_att2, x_att3])\n",
    "    \n",
    "    # Enhanced classifier with skip connection\n",
    "    x_skip = x\n",
    "    x = Dense(512, use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(256, use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x_skip = Dense(256, kernel_regularizer=l2(wd))(x_skip)\n",
    "    x = Add()([x, x_skip])\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = Dense(128, use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    out = Dense(n_classes, activation='softmax', kernel_regularizer=l2(wd))(x)\n",
    "    return Model(inp, out)\n",
    "    \n",
    "tmp_model = build_two_branch_model_v2(127,7,325,18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:35:03.055984Z",
     "iopub.status.busy": "2025-07-11T16:35:03.055702Z",
     "iopub.status.idle": "2025-07-11T17:30:18.795423Z",
     "shell.execute_reply": "2025-07-11T17:30:18.793932Z",
     "shell.execute_reply.started": "2025-07-11T16:35:03.055961Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Removing gravity and calculating linear acceleration features...\n",
      "IMU features: 24 | THM + ToF features: 55 | Total: 79\n",
      "Building sequences...\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "n_splits = 5  # Number of folds for cross-validation\n",
    "gate_loss_weight = 0.1  # If using gated model\n",
    "masking_prob = 0.2  # For data augmentation\n",
    "\n",
    "if train:\n",
    "    print(\"Loading dataset...\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(raw_dir / \"train.csv\")\n",
    "    train_dem_df = pd.read_csv(raw_dir / \"train_demographics.csv\")\n",
    "    df = pd.merge(df, train_dem_df, on='subject', how='left')\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    df['gesture_int'] = le.fit_transform(df['gesture'])\n",
    "    gesture_classes = le.classes_\n",
    "    np.save(output_dir / \"gesture_classes.npy\", gesture_classes)\n",
    "    \n",
    "    # ===== PHYSICAL FEATURE ENGINEERING =====\n",
    "    print(\"Removing gravity and calculating linear acceleration features...\")\n",
    "    \n",
    "    # Process by sequence for efficiency\n",
    "    sequence_features = []\n",
    "    for seq_id, group in df.groupby('sequence_id'):\n",
    "        seq_data = group.copy()\n",
    "        \n",
    "        # Linear acceleration (gravity removed)\n",
    "        linear_accel = remove_gravity_from_acc(\n",
    "            seq_data[['acc_x', 'acc_y', 'acc_z']], \n",
    "            seq_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "        )\n",
    "        seq_data['linear_acc_x'] = linear_accel[:, 0]\n",
    "        seq_data['linear_acc_y'] = linear_accel[:, 1]\n",
    "        seq_data['linear_acc_z'] = linear_accel[:, 2]\n",
    "        \n",
    "        # Acceleration magnitudes\n",
    "        seq_data['acc_mag'] = np.sqrt(\n",
    "            seq_data['acc_x']**2 + seq_data['acc_y']**2 + seq_data['acc_z']**2\n",
    "        )\n",
    "        seq_data['linear_acc_mag'] = np.sqrt(\n",
    "            seq_data['linear_acc_x']**2 + seq_data['linear_acc_y']**2 + seq_data['linear_acc_z']**2\n",
    "        )\n",
    "        seq_data['linear_acc_mag_jerk'] = seq_data['linear_acc_mag'].diff().fillna(0)\n",
    "        \n",
    "        # Jerk features\n",
    "        seq_data['jerk_x'] = seq_data['linear_acc_x'].diff() * 200  # 200Hz\n",
    "        seq_data['jerk_y'] = seq_data['linear_acc_y'].diff() * 200\n",
    "        seq_data['jerk_z'] = seq_data['linear_acc_z'].diff() * 200\n",
    "        seq_data[['jerk_x', 'jerk_y', 'jerk_z']] = seq_data[['jerk_x', 'jerk_y', 'jerk_z']].fillna(0)\n",
    "        \n",
    "        # Angular velocity\n",
    "        angular_vel = calculate_angular_velocity_from_quat(\n",
    "            seq_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "        )\n",
    "        seq_data['angular_vel_x'] = angular_vel[:, 0]\n",
    "        seq_data['angular_vel_y'] = angular_vel[:, 1]\n",
    "        seq_data['angular_vel_z'] = angular_vel[:, 2]\n",
    "        seq_data['angular_vel_mag'] = np.sqrt(\n",
    "            angular_vel[:, 0]**2 + angular_vel[:, 1]**2 + angular_vel[:, 2]**2\n",
    "        )\n",
    "        \n",
    "        # Angular distance\n",
    "        seq_data['angular_distance'] = calculate_angular_distance(\n",
    "            seq_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "        )\n",
    "        \n",
    "        # Angular acceleration\n",
    "        seq_data['angular_acc_x'] = seq_data['angular_vel_x'].diff() * 200\n",
    "        seq_data['angular_acc_y'] = seq_data['angular_vel_y'].diff() * 200\n",
    "        seq_data['angular_acc_z'] = seq_data['angular_vel_z'].diff() * 200\n",
    "        seq_data[['angular_acc_x', 'angular_acc_y', 'angular_acc_z']] = \\\n",
    "            seq_data[['angular_acc_x', 'angular_acc_y', 'angular_acc_z']].fillna(0)\n",
    "        \n",
    "        sequence_features.append(seq_data)\n",
    "    \n",
    "    # Combine all sequences\n",
    "    df = pd.concat(sequence_features, ignore_index=True)\n",
    "    \n",
    "    # ===== FEATURE COLUMN ORGANIZATION =====\n",
    "    # IMU features (enhanced)\n",
    "    imu_cols_base = ['acc_x', 'acc_y', 'acc_z', \n",
    "                     'linear_acc_x', 'linear_acc_y', 'linear_acc_z',\n",
    "                     'rot_x', 'rot_y', 'rot_z', 'rot_w']\n",
    "    \n",
    "    imu_engineered = ['acc_mag', 'linear_acc_mag', 'linear_acc_mag_jerk',\n",
    "                      'jerk_x', 'jerk_y', 'jerk_z',\n",
    "                      'angular_vel_x', 'angular_vel_y', 'angular_vel_z', 'angular_vel_mag',\n",
    "                      'angular_distance',\n",
    "                      'angular_acc_x', 'angular_acc_y', 'angular_acc_z']\n",
    "    \n",
    "    imu_cols = list(dict.fromkeys(imu_cols_base + imu_engineered))\n",
    "    \n",
    "    # Thermal columns\n",
    "    thm_cols = [c for c in df.columns if c.startswith('thm_')]\n",
    "    \n",
    "    # ToF aggregated columns (will be computed per sequence)\n",
    "    tof_aggregated_cols = []\n",
    "    for i in range(1, 6):\n",
    "        tof_aggregated_cols.extend([\n",
    "            f'tof_{i}_mean', f'tof_{i}_std', f'tof_{i}_min', f'tof_{i}_max',\n",
    "            f'tof_{i}_median', f'tof_{i}_q25', f'tof_{i}_q75', \n",
    "            f'tof_{i}_range', f'tof_{i}_iqr', f'tof_{i}_valid_count'\n",
    "        ])\n",
    "    \n",
    "    # All features\n",
    "    final_feature_cols = imu_cols + thm_cols + tof_aggregated_cols\n",
    "    imu_dim = len(imu_cols)\n",
    "    tof_thm_dim = len(thm_cols) + len(tof_aggregated_cols)\n",
    "    \n",
    "    print(f\"IMU features: {imu_dim} | THM + ToF features: {tof_thm_dim} | Total: {len(final_feature_cols)}\")\n",
    "    np.save(output_dir / \"feature_cols.npy\", np.array(final_feature_cols))\n",
    "    \n",
    "    # ===== BUILD SEQUENCES =====\n",
    "    print(\"Building sequences...\")\n",
    "    seq_gp = df.groupby('sequence_id')\n",
    "    X_list_unscaled, y_list_int, groups_list, lens = [], [], [], []\n",
    "    \n",
    "    for seq_id, seq_df in seq_gp:\n",
    "        seq_df_copy = seq_df.copy()\n",
    "        \n",
    "        # Compute ToF aggregations\n",
    "        for i in range(1, 6):\n",
    "            pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "            tof_data = seq_df_copy[pixel_cols].replace(-1, np.nan)\n",
    "            \n",
    "            # Basic stats\n",
    "            seq_df_copy[f'tof_{i}_mean'] = tof_data.mean(axis=1)\n",
    "            seq_df_copy[f'tof_{i}_std'] = tof_data.std(axis=1)\n",
    "            seq_df_copy[f'tof_{i}_min'] = tof_data.min(axis=1)\n",
    "            seq_df_copy[f'tof_{i}_max'] = tof_data.max(axis=1)\n",
    "            \n",
    "            # Enhanced stats\n",
    "            seq_df_copy[f'tof_{i}_median'] = tof_data.median(axis=1)\n",
    "            seq_df_copy[f'tof_{i}_q25'] = tof_data.quantile(0.25, axis=1)\n",
    "            seq_df_copy[f'tof_{i}_q75'] = tof_data.quantile(0.75, axis=1)\n",
    "            seq_df_copy[f'tof_{i}_range'] = seq_df_copy[f'tof_{i}_max'] - seq_df_copy[f'tof_{i}_min']\n",
    "            seq_df_copy[f'tof_{i}_iqr'] = seq_df_copy[f'tof_{i}_q75'] - seq_df_copy[f'tof_{i}_q25']\n",
    "            seq_df_copy[f'tof_{i}_valid_count'] = tof_data.notna().sum(axis=1)\n",
    "        \n",
    "        # Extract features and labels\n",
    "        X_list_unscaled.append(\n",
    "            seq_df_copy[final_feature_cols].ffill().bfill().fillna(0).values.astype('float32')\n",
    "        )\n",
    "        y_list_int.append(seq_df_copy['gesture_int'].iloc[0])\n",
    "        groups_list.append(seq_df_copy['subject'].iloc[0])\n",
    "        lens.append(len(seq_df_copy))\n",
    "    \n",
    "    # ===== SCALING AND PADDING =====\n",
    "    print(\"Fitting StandardScaler...\")\n",
    "    all_steps_concatenated = np.concatenate(X_list_unscaled, axis=0)\n",
    "    scaler = StandardScaler().fit(all_steps_concatenated)\n",
    "    joblib.dump(scaler, output_dir / \"scaler.pkl\")\n",
    "    \n",
    "    print(\"Scaling and padding sequences...\")\n",
    "    X_scaled_list = [scaler.transform(x_seq) for x_seq in X_list_unscaled]\n",
    "    \n",
    "    # Determine padding length\n",
    "    pad_len = int(np.percentile(lens, pad_percentile))\n",
    "    np.save(output_dir / \"sequence_maxlen.npy\", pad_len)\n",
    "    print(f\"Padding sequences to length {pad_len}\")\n",
    "    \n",
    "    # Pad sequences\n",
    "    X = pad_sequences(X_scaled_list, maxlen=pad_len, padding='post', \n",
    "                      truncating='post', dtype='float32')\n",
    "    y_stratify = np.array(y_list_int)\n",
    "    groups = np.array(groups_list)\n",
    "    y = to_categorical(y_list_int, num_classes=len(le.classes_))\n",
    "    \n",
    "    print(f\"Final data shape: X={X.shape}, y={y.shape}\")\n",
    "    \n",
    "    # ===== STRATIFIED GROUP K-FOLD CROSS-VALIDATION =====\n",
    "    print(f\"\\n  Starting training with Stratified Group K-Fold CV (n_splits={n_splits})...\")\n",
    "    sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    oof_preds = np.zeros_like(y, dtype='float32')\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(sgkf.split(X, y_stratify, groups)):\n",
    "        print(f\"\\n{'='*20} FOLD {fold+1}/{n_splits} {'='*20}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_tr, X_val = X[train_idx], X[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        print(f\"  Train: {len(X_tr)} samples | Val: {len(X_val)} samples\")\n",
    "        \n",
    "        # Build model\n",
    "        model = build_two_branch_model_v2(\n",
    "            pad_len=pad_len, \n",
    "            imu_dim=imu_dim, \n",
    "            tof_dim=tof_thm_dim, \n",
    "            n_classes=len(le.classes_), \n",
    "            wd=wd\n",
    "        )\n",
    "        \n",
    "        # Compile with label smoothing\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=lr_init),\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Class weights for imbalanced data\n",
    "        class_weights = compute_class_weight(\n",
    "            'balanced', \n",
    "            classes=np.arange(len(le.classes_)), \n",
    "            y=y_tr.argmax(1)\n",
    "        )\n",
    "        class_weight_dict = dict(enumerate(class_weights))\n",
    "        \n",
    "        # Data generators\n",
    "        train_gen = EnhancedMixupGenerator(\n",
    "            X_tr, y_tr, \n",
    "            batch_size=batch_size, \n",
    "            alpha=mixup_alpha, \n",
    "            augment=True\n",
    "        )\n",
    "        \n",
    "        # Callbacks with fold-specific naming\n",
    "        fold_callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss', \n",
    "                patience=patience, \n",
    "                restore_best_weights=True, \n",
    "                verbose=1\n",
    "            ),\n",
    "            lr_scheduler,\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                factor=0.5, \n",
    "                patience=10, \n",
    "                min_lr=1e-7, \n",
    "                verbose=1\n",
    "            ),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                str(output_dir / f'model_fold_{fold}_best.h5'),\n",
    "                save_best_only=True,\n",
    "                monitor='val_loss',\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train\n",
    "        history = model.fit(\n",
    "            train_gen,\n",
    "            epochs=epochs,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=fold_callbacks,\n",
    "            class_weight=class_weight_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Save final model for this fold\n",
    "        model.save(output_dir / f\"model_fold_{fold}_final.h5\")\n",
    "        \n",
    "        # Get predictions for OOF\n",
    "        preds_val = model.predict(X_val, verbose=0)\n",
    "        oof_preds[val_idx] = preds_val\n",
    "        \n",
    "        # Calculate fold score\n",
    "        fold_acc = np.mean(preds_val.argmax(1) == y_val.argmax(1))\n",
    "        fold_scores.append(fold_acc)\n",
    "        print(f\"Fold {fold+1} Validation Accuracy: {fold_acc:.4f}\")\n",
    "        \n",
    "        # Clear session to free memory\n",
    "        tf.keras.backend.clear_session()\n",
    "    \n",
    "    # ===== CALCULATE OOF SCORE =====\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Training Complete!\")\n",
    "    print(f\"Average Fold Score: {np.mean(fold_scores):.4f} (+/- {np.std(fold_scores):.4f})\")\n",
    "    \n",
    "    # Calculate overall OOF accuracy\n",
    "    oof_acc = np.mean(oof_preds.argmax(1) == y.argmax(1))\n",
    "    print(f\"Overall OOF Accuracy: {oof_acc:.4f}\")\n",
    "    \n",
    "    # Save OOF predictions\n",
    "    np.save(output_dir / \"oof_predictions.npy\", oof_preds)\n",
    "    \n",
    "    # If competition metric is available\n",
    "    try:\n",
    "        true_oof_int = y.argmax(1)\n",
    "        pred_oof_int = oof_preds.argmax(1)\n",
    "        \n",
    "        h_f1_oof = CompetitionMetric().calculate_hierarchical_f1(\n",
    "            pd.DataFrame({'gesture': le.classes_[true_oof_int]}),\n",
    "            pd.DataFrame({'gesture': le.classes_[pred_oof_int]})\n",
    "        )\n",
    "        print(f\"Overall OOF Hierarchical F1 Score: {h_f1_oof:.4f}\")\n",
    "    except ImportError:\n",
    "        print(\"Competition metric not available - skipping H-F1 calculation\")\n",
    "    \n",
    "    print(\"Training pipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
