{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-11T15:18:11.785890Z",
     "iopub.status.busy": "2025-07-11T15:18:11.785298Z",
     "iopub.status.idle": "2025-07-11T15:18:25.841345Z",
     "shell.execute_reply": "2025-07-11T15:18:25.840555Z",
     "shell.execute_reply.started": "2025-07-11T15:18:11.785859Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, json, joblib, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings \n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence, to_categorical, pad_sequences\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, BatchNormalization, Activation, add, MaxPooling1D, Dropout,\n",
    "    Bidirectional, LSTM, GlobalAveragePooling1D, Dense, Multiply, Reshape,\n",
    "    Lambda, Concatenate, GRU, GaussianNoise, Add, GlobalMaxPooling1D,\n",
    "    MultiHeadAttention, LayerNormalization\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import polars as pl\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "# Enable mixed precision training for faster computation\n",
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T15:18:25.842891Z",
     "iopub.status.busy": "2025-07-11T15:18:25.842429Z",
     "iopub.status.idle": "2025-07-11T15:18:25.847306Z",
     "shell.execute_reply": "2025-07-11T15:18:25.846547Z",
     "shell.execute_reply.started": "2025-07-11T15:18:25.842872Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.experimental.numpy.random.seed(seed)\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T16:34:55.557690Z",
     "iopub.status.busy": "2025-07-11T16:34:55.557127Z",
     "iopub.status.idle": "2025-07-11T16:34:55.562441Z",
     "shell.execute_reply": "2025-07-11T16:34:55.561597Z",
     "shell.execute_reply.started": "2025-07-11T16:34:55.557669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = True                    \n",
    "raw_dir = Path(\"input\")\n",
    "pretrained_dir = Path(\"input/cmi-d-111\")\n",
    "output_dir = Path(\"./\")                                    \n",
    "batch_size = 64 \n",
    "pad_percentile = 95\n",
    "lr_init = 5e-4\n",
    "wd = 3e-3\n",
    "mixup_alpha = 0.4\n",
    "epochs = 160  \n",
    "patience = 40 \n",
    "n_splits = 5 \n",
    "patience = 40\n",
    "\n",
    "print(\"Imports ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T15:18:25.879075Z",
     "iopub.status.busy": "2025-07-11T15:18:25.878765Z",
     "iopub.status.idle": "2025-07-11T15:18:25.891019Z",
     "shell.execute_reply": "2025-07-11T15:18:25.890484Z",
     "shell.execute_reply.started": "2025-07-11T15:18:25.879048Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Tensor Manipulations\n",
    "def time_sum(x):\n",
    "    return K.sum(x, axis=1)\n",
    "\n",
    "def squeeze_last_axis(x):\n",
    "    return tf.squeeze(x, axis=-1)\n",
    "\n",
    "def expand_last_axis(x):\n",
    "    return tf.expand_dims(x, axis=-1)\n",
    "\n",
    "def se_block(x, reduction=8):\n",
    "    ch = x.shape[-1]\n",
    "    se = GlobalAveragePooling1D()(x)\n",
    "    se = Dense(ch // reduction, activation='relu')(se)\n",
    "    se = Dense(ch, activation='sigmoid')(se)\n",
    "    se = Reshape((1, ch))(se)\n",
    "    return Multiply()([x, se])\n",
    "\n",
    "# Residual CNN Block with SE\n",
    "def residual_se_cnn_block(x, filters, kernel_size, pool_size=2, drop=0.3, wd=1e-4):\n",
    "    shortcut = x\n",
    "    for _ in range(2):\n",
    "        x = Conv1D(filters, kernel_size, padding='same', use_bias=False,\n",
    "                   kernel_regularizer=l2(wd))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "    x = se_block(x)\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, 1, padding='same', use_bias=False,\n",
    "                          kernel_regularizer=l2(wd))(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    x = add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(pool_size)(x)\n",
    "    x = Dropout(drop)(x)\n",
    "    return x\n",
    "\n",
    "def attention_layer(inputs):\n",
    "    score = Dense(1, activation='tanh')(inputs)\n",
    "    score = Lambda(squeeze_last_axis)(score)\n",
    "    weights = Activation('softmax')(score)\n",
    "    weights = Lambda(expand_last_axis)(weights)\n",
    "    context = Multiply()([inputs, weights])\n",
    "    context = Lambda(time_sum)(context)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized physics calculations using vectorization\n",
    "def remove_gravity_from_acc_vectorized(acc_values, quat_values):\n",
    "    \"\"\"Vectorized gravity removal for better performance\"\"\"\n",
    "    num_samples = acc_values.shape[0]\n",
    "    linear_accel = acc_values.copy()\n",
    "    \n",
    "    # Filter valid quaternions\n",
    "    valid_mask = ~(np.any(np.isnan(quat_values), axis=1) | \n",
    "                   np.all(np.isclose(quat_values, 0), axis=1))\n",
    "    \n",
    "    if np.any(valid_mask):\n",
    "        # Process all valid quaternions at once\n",
    "        valid_quats = quat_values[valid_mask]\n",
    "        \n",
    "        # Batch rotation computation\n",
    "        try:\n",
    "            rotations = R.from_quat(valid_quats)\n",
    "            gravity_world = np.array([0, 0, 9.81])\n",
    "            \n",
    "            # Apply rotations in batch\n",
    "            gravity_sensor_frames = rotations.apply(gravity_world, inverse=True)\n",
    "            linear_accel[valid_mask] = acc_values[valid_mask] - gravity_sensor_frames\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    return linear_accel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angular_velocity_vectorized(quat_values, time_delta=1/200):\n",
    "    \"\"\"Vectorized angular velocity calculation\"\"\"\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_vel = np.zeros((num_samples, 3))\n",
    "    \n",
    "    if num_samples < 2:\n",
    "        return angular_vel\n",
    "        \n",
    "    # Process in chunks for memory efficiency\n",
    "    chunk_size = 1000\n",
    "    for start in range(0, num_samples - 1, chunk_size):\n",
    "        end = min(start + chunk_size, num_samples - 1)\n",
    "        \n",
    "        q_t = quat_values[start:end]\n",
    "        q_t_plus_dt = quat_values[start+1:end+1]\n",
    "        \n",
    "        # Find valid pairs\n",
    "        valid_mask = ~(np.any(np.isnan(q_t), axis=1) | \n",
    "                      np.all(np.isclose(q_t, 0), axis=1) |\n",
    "                      np.any(np.isnan(q_t_plus_dt), axis=1) | \n",
    "                      np.all(np.isclose(q_t_plus_dt, 0), axis=1))\n",
    "        \n",
    "        if np.any(valid_mask):\n",
    "            try:\n",
    "                valid_indices = np.where(valid_mask)[0]\n",
    "                rot_t = R.from_quat(q_t[valid_mask])\n",
    "                rot_t_plus_dt = R.from_quat(q_t_plus_dt[valid_mask])\n",
    "                \n",
    "                # Batch computation\n",
    "                delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "                angular_vel[start + valid_indices] = delta_rot.as_rotvec() / time_delta\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "    return angular_vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified MixUp generator\n",
    "class FastMixupGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size, alpha=0.4):\n",
    "        self.X, self.y = X, y\n",
    "        self.batch = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.indices = np.arange(len(X))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch))\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        idx = self.indices[i*self.batch:(i+1)*self.batch]\n",
    "        Xb, yb = self.X[idx], self.y[idx]\n",
    "        \n",
    "        # Simple mixup\n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        perm = np.random.permutation(len(Xb))\n",
    "        X_mix = lam * Xb + (1-lam) * Xb[perm]\n",
    "        y_mix = lam * yb + (1-lam) * yb[perm]\n",
    "        \n",
    "        return X_mix.astype('float32'), y_mix.astype('float32')\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified model architecture (slightly reduced complexity)\n",
    "def build_two_branch_model_optimized(pad_len, imu_dim, tof_dim, n_classes, wd=1e-4):\n",
    "    inp = Input(shape=(pad_len, imu_dim+tof_dim))\n",
    "    imu = Lambda(lambda t: t[:, :, :imu_dim])(inp)\n",
    "    tof = Lambda(lambda t: t[:, :, imu_dim:])(inp)\n",
    "    \n",
    "    # IMU branch - simplified\n",
    "    x1 = residual_se_cnn_block(imu, 64, 5, drop=0.1, wd=wd)\n",
    "    x1 = residual_se_cnn_block(x1, 128, 5, drop=0.1, wd=wd)\n",
    "    \n",
    "    # TOF branch - simplified\n",
    "    x2 = Conv1D(128, 1, padding='same', use_bias=False, kernel_regularizer=l2(wd))(tof)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Activation('relu')(x2)\n",
    "    x2 = residual_se_cnn_block(x2, 192, 3, drop=0.2, wd=wd)\n",
    "    x2 = residual_se_cnn_block(x2, 256, 3, drop=0.2, wd=wd)\n",
    "    \n",
    "    # Simple concatenation\n",
    "    merged = Concatenate()([x1, x2])\n",
    "    \n",
    "    # Single RNN layer instead of multiple\n",
    "    x = Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(wd),\n",
    "                         dropout=0.2, recurrent_dropout=0.2))(merged)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Attention\n",
    "    x_att = attention_layer(x)\n",
    "    x_pool = GlobalAveragePooling1D()(x)\n",
    "    x = Concatenate()([x_att, x_pool])\n",
    "    \n",
    "    # Simplified classifier\n",
    "    x = Dense(256, use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = Dense(128, use_bias=False, kernel_regularizer=l2(wd))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Output layer with float32 for stability with mixed precision\n",
    "    x = Dense(n_classes, kernel_regularizer=l2(wd), dtype='float32')(x)\n",
    "    out = Activation('softmax', dtype='float32')(x)\n",
    "    \n",
    "    return Model(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel processing for sequence features\n",
    "def process_sequence(seq_data):\n",
    "    \"\"\"Process a single sequence with all feature engineering\"\"\"\n",
    "    seq_data = seq_data.copy()\n",
    "    \n",
    "    # Get numpy arrays for faster processing\n",
    "    acc_values = seq_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    quat_values = seq_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    \n",
    "    # Linear acceleration\n",
    "    linear_accel = remove_gravity_from_acc_vectorized(acc_values, quat_values)\n",
    "    seq_data['linear_acc_x'] = linear_accel[:, 0]\n",
    "    seq_data['linear_acc_y'] = linear_accel[:, 1]\n",
    "    seq_data['linear_acc_z'] = linear_accel[:, 2]\n",
    "    \n",
    "    # Magnitudes\n",
    "    seq_data['acc_mag'] = np.linalg.norm(acc_values, axis=1)\n",
    "    seq_data['linear_acc_mag'] = np.linalg.norm(linear_accel, axis=1)\n",
    "    \n",
    "    # Jerk (simplified)\n",
    "    seq_data['linear_acc_mag_jerk'] = np.gradient(seq_data['linear_acc_mag']) * 200\n",
    "    \n",
    "    # Angular velocity\n",
    "    angular_vel = calculate_angular_velocity_vectorized(quat_values)\n",
    "    seq_data['angular_vel_x'] = angular_vel[:, 0]\n",
    "    seq_data['angular_vel_y'] = angular_vel[:, 1]\n",
    "    seq_data['angular_vel_z'] = angular_vel[:, 2]\n",
    "    \n",
    "    # ToF aggregations (vectorized)\n",
    "    for i in range(1, 6):\n",
    "        pixel_cols = [f\"tof_{i}_v{p}\" for p in range(64)]\n",
    "        tof_data = seq_data[pixel_cols].values\n",
    "        tof_data[tof_data == -1] = np.nan\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            seq_data[f'tof_{i}_mean'] = np.nanmean(tof_data, axis=1)\n",
    "            seq_data[f'tof_{i}_std'] = np.nanstd(tof_data, axis=1)\n",
    "            seq_data[f'tof_{i}_min'] = np.nanmin(tof_data, axis=1)\n",
    "            seq_data[f'tof_{i}_max'] = np.nanmax(tof_data, axis=1)\n",
    "    \n",
    "    return seq_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    print(\"Loading dataset...\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(raw_dir / \"train.csv\")\n",
    "    train_dem_df = pd.read_csv(raw_dir / \"train_demographics.csv\")\n",
    "    df = pd.merge(df, train_dem_df, on='subject', how='left')\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    df['gesture_int'] = le.fit_transform(df['gesture'])\n",
    "    gesture_classes = le.classes_\n",
    "    np.save(output_dir / \"gesture_classes.npy\", gesture_classes)\n",
    "    \n",
    "    print(\"Processing sequences with parallel computation...\")\n",
    "    \n",
    "    # Group by sequence\n",
    "    sequences = [group for _, group in df.groupby('sequence_id')]\n",
    "    \n",
    "    # Process in parallel\n",
    "    n_cores = multiprocessing.cpu_count()\n",
    "    print(f\"Using {n_cores} cores for parallel processing\")\n",
    "    \n",
    "    processed_sequences = Parallel(n_jobs=n_cores)(\n",
    "        delayed(process_sequence)(seq) for seq in sequences\n",
    "    )\n",
    "    \n",
    "    # Combine processed sequences\n",
    "    df = pd.concat(processed_sequences, ignore_index=True)\n",
    "    \n",
    "    # Define feature columns\n",
    "    imu_cols = ['acc_x', 'acc_y', 'acc_z', \n",
    "                'linear_acc_x', 'linear_acc_y', 'linear_acc_z',\n",
    "                'rot_x', 'rot_y', 'rot_z', 'rot_w',\n",
    "                'acc_mag', 'linear_acc_mag', 'linear_acc_mag_jerk',\n",
    "                'angular_vel_x', 'angular_vel_y', 'angular_vel_z']\n",
    "    \n",
    "    thm_cols = [c for c in df.columns if c.startswith('thm_')]\n",
    "    tof_aggregated_cols = []\n",
    "    for i in range(1, 6):\n",
    "        tof_aggregated_cols.extend([\n",
    "            f'tof_{i}_mean', f'tof_{i}_std', f'tof_{i}_min', f'tof_{i}_max'\n",
    "        ])\n",
    "    \n",
    "    final_feature_cols = imu_cols + thm_cols + tof_aggregated_cols\n",
    "    imu_dim = len(imu_cols)\n",
    "    tof_thm_dim = len(thm_cols) + len(tof_aggregated_cols)\n",
    "    \n",
    "    print(f\"IMU features: {imu_dim} | THM + ToF features: {tof_thm_dim}\")\n",
    "    np.save(output_dir / \"feature_cols.npy\", np.array(final_feature_cols))\n",
    "    \n",
    "    # Build sequences efficiently\n",
    "    print(\"Building sequences...\")\n",
    "    seq_gp = df.groupby('sequence_id')\n",
    "    X_list_unscaled = []\n",
    "    y_list_int = []\n",
    "    groups_list = []\n",
    "    lens = []\n",
    "    \n",
    "    for seq_id, seq_df in seq_gp:\n",
    "        X_list_unscaled.append(\n",
    "            seq_df[final_feature_cols].fillna(0).values.astype('float32')\n",
    "        )\n",
    "        y_list_int.append(seq_df['gesture_int'].iloc[0])\n",
    "        groups_list.append(seq_df['subject'].iloc[0])\n",
    "        lens.append(len(seq_df))\n",
    "    \n",
    "    # Scaling\n",
    "    print(\"Fitting StandardScaler...\")\n",
    "    all_steps_concatenated = np.concatenate(X_list_unscaled, axis=0)\n",
    "    scaler = StandardScaler().fit(all_steps_concatenated)\n",
    "    joblib.dump(scaler, output_dir / \"scaler.pkl\")\n",
    "    \n",
    "    # Scale and pad\n",
    "    X_scaled_list = [scaler.transform(x_seq) for x_seq in X_list_unscaled]\n",
    "    pad_len = int(np.percentile(lens, pad_percentile))\n",
    "    np.save(output_dir / \"sequence_maxlen.npy\", pad_len)\n",
    "    \n",
    "    X = pad_sequences(X_scaled_list, maxlen=pad_len, padding='post', \n",
    "                      truncating='post', dtype='float32')\n",
    "    y_stratify = np.array(y_list_int)\n",
    "    groups = np.array(groups_list)\n",
    "    y = to_categorical(y_list_int, num_classes=len(le.classes_))\n",
    "    \n",
    "    print(f\"Final data shape: X={X.shape}, y={y.shape}\")\n",
    "    \n",
    "    # Cross-validation with reduced folds\n",
    "    print(f\"\\nStarting training with {n_splits}-fold CV...\")\n",
    "    sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(sgkf.split(X, y_stratify, groups)):\n",
    "        print(f\"\\n{'='*20} FOLD {fold+1}/{n_splits} {'='*20}\")\n",
    "        \n",
    "        X_tr, X_val = X[train_idx], X[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Build model\n",
    "        model = build_two_branch_model_optimized(\n",
    "            pad_len=pad_len, \n",
    "            imu_dim=imu_dim, \n",
    "            tof_dim=tof_thm_dim, \n",
    "            n_classes=len(le.classes_), \n",
    "            wd=wd\n",
    "        )\n",
    "        \n",
    "        # Compile with mixed precision optimizer\n",
    "        opt = Adam(learning_rate=lr_init)\n",
    "        opt = mixed_precision.LossScaleOptimizer(opt)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=opt,\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Simplified callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss', \n",
    "                patience=patience, \n",
    "                restore_best_weights=True, \n",
    "                verbose=1\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                factor=0.5, \n",
    "                patience=10, \n",
    "                min_lr=1e-6, \n",
    "                verbose=1\n",
    "            ),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                str(output_dir / f'model_fold_{fold}_best.h5'),\n",
    "                save_best_only=True,\n",
    "                monitor='val_loss'\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train with larger batch size\n",
    "        train_gen = FastMixupGenerator(\n",
    "            X_tr, y_tr, \n",
    "            batch_size=batch_size, \n",
    "            alpha=mixup_alpha\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_gen,\n",
    "            epochs=epochs,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Save model\n",
    "        model.save(output_dir / f\"model_fold_{fold}_final.h5\")\n",
    "        \n",
    "        # Clear session\n",
    "        tf.keras.backend.clear_session()\n",
    "    \n",
    "    print(\"\\nTraining completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
